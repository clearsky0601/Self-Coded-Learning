{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 PyTorch 自动求导机制 (Autograd) 深度解析\n",
    "\n",
    "## 📚 本节学习目标\n",
    "- 深入理解自动求导的原理和重要性\n",
    "- 掌握计算图的概念和构建过程\n",
    "- 熟练使用 `requires_grad` 和 `backward()`\n",
    "- 理解梯度的计算、存储和清零\n",
    "- 掌握链式法则在自动求导中的应用\n",
    "- 学会处理复杂的梯度计算场景\n",
    "- 了解自动求导的高级特性和注意事项\n",
    "\n",
    "---\n",
    "\n",
    "## 🤔 为什么需要自动求导？\n",
    "\n",
    "在深度学习中，我们需要计算损失函数对模型参数的梯度来更新参数。手动计算梯度既容易出错又极其复杂，尤其是在深层神经网络中。\n",
    "\n",
    "**自动求导的优势：**\n",
    "- 🎯 **自动化**：无需手动推导复杂的梯度公式\n",
    "- ✅ **准确性**：避免人为计算错误\n",
    "- 🚀 **效率**：优化的计算图执行\n",
    "- 🔧 **灵活性**：支持动态计算图\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到路径\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from utils.common import get_device, print_tensor_info, set_seed\n",
    "\n",
    "# 设置随机种子确保结果可重现\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"🔥 PyTorch 版本: {torch.__version__}\")\n",
    "device = get_device()\n",
    "\n",
    "# 设置matplotlib中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 1. 自动求导基础概念\n",
    "\n",
    "### 1.1 什么是计算图？\n",
    "\n",
    "**计算图(Computational Graph)** 是一个有向无环图，用来表示数学运算的执行过程：\n",
    "- **节点**：表示变量或运算\n",
    "- **边**：表示数据流向\n",
    "- **前向传播**：从输入到输出计算结果\n",
    "- **反向传播**：从输出到输入计算梯度\n",
    "\n",
    "### 🌰 举个生活化的例子\n",
    "假设你在做菜，计算总成本：\n",
    "```\n",
    "蔬菜价格(a) = 10元    肉价格(b) = 20元\n",
    "    ↓                   ↓\n",
    "  蔬菜成本 = a × 2    肉成本 = b × 1.5\n",
    "    ↓                   ↓\n",
    "      总成本(c) = 蔬菜成本 + 肉成本\n",
    "```\n",
    "\n",
    "如果蔬菜价格变化1元，总成本变化多少？这就是求梯度 ∂c/∂a！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 自动求导基础演示\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 创建需要梯度的张量 - 这是关键！\n",
    "# requires_grad=True 告诉PyTorch：\"我需要计算这个张量的梯度\"\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "print(f\"📊 输入变量:\")\n",
    "print(f\"x = {x}, requires_grad = {x.requires_grad}\")\n",
    "print(f\"y = {y}, requires_grad = {y.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# 定义一个简单的计算：z = x² + 2xy + y²\n",
    "# 这相当于 z = (x + y)²\n",
    "z = x**2 + 2*x*y + y**2\n",
    "\n",
    "print(f\"🧮 计算过程:\")\n",
    "print(f\"z = x² + 2xy + y² = {x}² + 2×{x}×{y} + {y}² = {z}\")\n",
    "print(f\"z.requires_grad = {z.requires_grad}  # 自动继承\")\n",
    "print(f\"z.grad_fn = {z.grad_fn}  # 记录如何计算得到z\")\n",
    "print()\n",
    "\n",
    "# 计算梯度 - 反向传播！\n",
    "print(f\"🔄 开始反向传播...\")\n",
    "z.backward()  # 计算 dz/dx 和 dz/dy\n",
    "\n",
    "print(f\"✅ 梯度计算完成!\")\n",
    "print(f\"dz/dx = {x.grad}  # 理论值: 2x + 2y = 2×{x.item()} + 2×{y.item()} = {2*x.item() + 2*y.item()}\")\n",
    "print(f\"dz/dy = {y.grad}  # 理论值: 2y + 2x = 2×{y.item()} + 2×{x.item()} = {2*y.item() + 2*x.item()}\")\n",
    "\n",
    "print(\"\\n💡 观察：自动求导的结果与理论计算完全一致！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 requires_grad 的作用机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 requires_grad 详解\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. 默认情况：不需要梯度\n",
    "a = torch.tensor(5.0)  # requires_grad=False (默认)\n",
    "b = torch.tensor(3.0, requires_grad=True)  # 显式设置为True\n",
    "\n",
    "print(f\"📊 基础张量:\")\n",
    "print(f\"a = {a}, requires_grad = {a.requires_grad}\")\n",
    "print(f\"b = {b}, requires_grad = {b.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# 2. 传播规则：只要有一个操作数需要梯度，结果就需要梯度\n",
    "c = a + b  # a不需要梯度，b需要梯度 → c需要梯度\n",
    "d = a * 2  # a不需要梯度，常数运算 → d不需要梯度\n",
    "\n",
    "print(f\"🔄 传播规则演示:\")\n",
    "print(f\"c = a + b = {c}, requires_grad = {c.requires_grad}  # 继承自b\")\n",
    "print(f\"d = a * 2 = {d}, requires_grad = {d.requires_grad}  # a不需要梯度\")\n",
    "print()\n",
    "\n",
    "# 3. 动态控制：可以随时开启/关闭梯度计算\n",
    "print(f\"🎛️ 动态控制梯度:\")\n",
    "\n",
    "# 开启梯度\n",
    "a.requires_grad_(True)  # 注意：带下划线的方法是就地操作\n",
    "print(f\"开启后 a.requires_grad = {a.requires_grad}\")\n",
    "\n",
    "# 关闭梯度\n",
    "b.requires_grad_(False)\n",
    "print(f\"关闭后 b.requires_grad = {b.requires_grad}\")\n",
    "\n",
    "# 4. 只有浮点数类型才能计算梯度\n",
    "try:\n",
    "    int_tensor = torch.tensor(5, requires_grad=True)  # 整数类型\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n❌ 错误: {e}\")\n",
    "    print(\"💡 解决方案: 使用 .float() 转换为浮点类型\")\n",
    "    int_tensor = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "    print(f\"✅ 修正后: {int_tensor}, dtype = {int_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 2. 计算图的构建和执行\n",
    "\n",
    "PyTorch使用**动态计算图**，这意味着图是在运行时构建的，每次前向传播都会重新构建图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧠 计算图构建过程详解\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 创建输入节点\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"🎯 步骤1: 创建输入节点\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.grad_fn = {x.grad_fn}  # 输入节点没有grad_fn\")\n",
    "print()\n",
    "\n",
    "# 逐步构建计算图\n",
    "print(f\"🔨 步骤2: 逐步构建计算图\")\n",
    "\n",
    "# 第一步：x² \n",
    "x_squared = x ** 2\n",
    "print(f\"y1 = x² = {x_squared}\")\n",
    "print(f\"y1.grad_fn = {x_squared.grad_fn}  # 记录这是幂运算\")\n",
    "\n",
    "# 第二步：2x²\n",
    "two_x_squared = 2 * x_squared \n",
    "print(f\"y2 = 2×y1 = {two_x_squared}\")\n",
    "print(f\"y2.grad_fn = {two_x_squared.grad_fn}  # 记录这是乘法运算\")\n",
    "\n",
    "# 第三步：2x² + 1\n",
    "final_result = two_x_squared + 1\n",
    "print(f\"y3 = y2 + 1 = {final_result}\")\n",
    "print(f\"y3.grad_fn = {final_result.grad_fn}  # 记录这是加法运算\")\n",
    "print()\n",
    "\n",
    "# 查看计算图的next_functions\n",
    "print(f\"🔍 计算图连接关系:\")\n",
    "print(f\"final_result 的前一个函数: {final_result.grad_fn.next_functions}\")\n",
    "print(f\"two_x_squared 的前一个函数: {two_x_squared.grad_fn.next_functions}\")\n",
    "print()\n",
    "\n",
    "# 反向传播\n",
    "print(f\"🔄 步骤3: 反向传播\")\n",
    "print(f\"计算 d(2x²+1)/dx 在 x={x.item()} 处的值\")\n",
    "print(f\"理论结果: d(2x²+1)/dx = 4x = 4×{x.item()} = {4*x.item()}\")\n",
    "\n",
    "final_result.backward()\n",
    "print(f\"自动求导结果: {x.grad}\")\n",
    "print(f\"✅ 验证通过: 理论值 = 自动求导值\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 可视化计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_computational_graph():\n",
    "    \"\"\"可视化一个简单的计算图过程\"\"\"\n",
    "    print(\"🎨 计算图可视化\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 重新创建变量（清除之前的梯度）\n",
    "    x = torch.tensor(3.0, requires_grad=True)\n",
    "    \n",
    "    print(\"📈 前向传播过程:\")\n",
    "    print(\"\")\n",
    "    print(\"    x = 3.0\")\n",
    "    print(\"    │\")\n",
    "    print(\"    ▼ (** 2)\")\n",
    "    \n",
    "    y = x ** 2\n",
    "    print(f\"    y = x² = {y.item()}\")\n",
    "    print(\"    │\")\n",
    "    print(\"    ▼ (* 2)\")\n",
    "    \n",
    "    z = 2 * y\n",
    "    print(f\"    z = 2y = {z.item()}\")\n",
    "    print(\"    │\")\n",
    "    print(\"    ▼ (+ 1)\")\n",
    "    \n",
    "    w = z + 1\n",
    "    print(f\"    w = z + 1 = {w.item()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📉 反向传播过程:\")\n",
    "    print(\"\")\n",
    "    w.backward()\n",
    "    \n",
    "    print(\"    dw/dw = 1\")\n",
    "    print(\"    ▲\")\n",
    "    print(\"    │ dw/dz = dw/dw × d(z+1)/dz = 1 × 1 = 1\")\n",
    "    print(\"    ▲\")\n",
    "    print(\"    │ dw/dy = dw/dz × d(2y)/dy = 1 × 2 = 2\")\n",
    "    print(\"    ▲\")\n",
    "    print(\"    │ dw/dx = dw/dy × d(x²)/dx = 2 × 2x = 2 × 2×3 = 12\")\n",
    "    print(f\"    最终梯度: {x.grad}\")\n",
    "    \n",
    "    return x.grad\n",
    "\n",
    "gradient = visualize_computational_graph()\n",
    "print(f\"\\n💡 这就是链式法则的自动应用！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ 3. 梯度的计算、存储和管理\n",
    "\n",
    "理解梯度如何计算、存储和管理是掌握自动求导的关键。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⚡ 梯度管理详解\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. 梯度的累积特性\n",
    "print(\"📈 梯度累积演示:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# 第一次计算\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"第一次: y1 = x², x.grad = {x.grad}\")\n",
    "\n",
    "# 第二次计算（不清零梯度）\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"第二次: y2 = x³, x.grad = {x.grad}  # 梯度累积了！\")\n",
    "print(f\"解释: 第一次梯度(2x=4) + 第二次梯度(3x²=12) = {4 + 12}\")\n",
    "print()\n",
    "\n",
    "# 2. 梯度清零 - 非常重要！\n",
    "print(\"🧹 梯度清零:\")\n",
    "x.grad.zero_()  # 就地清零\n",
    "# 或者使用: x.grad = None\n",
    "print(f\"清零后 x.grad = {x.grad}\")\n",
    "\n",
    "# 重新计算\n",
    "y3 = x ** 3\n",
    "y3.backward()\n",
    "print(f\"重新计算: y3 = x³, x.grad = {x.grad}  # 现在是正确的值\")\n",
    "print()\n",
    "\n",
    "# 3. 标量vs向量的反向传播\n",
    "print(\"📊 向量梯度计算:\")\n",
    "x_vec = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y_vec = x_vec ** 2  # [1, 4, 9]\n",
    "\n",
    "# 对于向量，需要提供gradient参数\n",
    "gradient_weights = torch.tensor([1.0, 1.0, 1.0])  # 每个元素的权重\n",
    "y_vec.backward(gradient_weights)\n",
    "\n",
    "print(f\"x_vec = {x_vec.data}\")\n",
    "print(f\"y_vec = x_vec² = {y_vec.data}\")\n",
    "print(f\"梯度 dy/dx = 2x = {x_vec.grad}\")\n",
    "print(f\"验证: [2×1, 2×2, 2×3] = [2, 4, 6] ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 梯度计算的高级特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 高级梯度特性\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. retain_graph：保留计算图\n",
    "print(\"🔄 retain_graph 使用:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y ** 2  # z = x⁴\n",
    "\n",
    "# 第一次反向传播，保留图\n",
    "z.backward(retain_graph=True)\n",
    "print(f\"第一次反向传播 dz/dx = {x.grad}  # 4x³ = 4×8 = 32\")\n",
    "\n",
    "# 第二次反向传播（如果不保留图，这里会出错）\n",
    "x.grad.zero_()  # 清零梯度\n",
    "z.backward()  # 再次计算（因为图被保留了）\n",
    "print(f\"第二次反向传播 dz/dx = {x.grad}  # 结果相同\")\n",
    "print()\n",
    "\n",
    "# 2. create_graph：创建梯度的梯度（二阶导数）\n",
    "print(\"📈 二阶导数计算:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3  # y = x³\n",
    "\n",
    "# 计算一阶导数\n",
    "grad_first = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"一阶导数 dy/dx = 3x² = {grad_first}\")\n",
    "\n",
    "# 计算二阶导数\n",
    "grad_second = torch.autograd.grad(grad_first, x)[0]\n",
    "print(f\"二阶导数 d²y/dx² = 6x = {grad_second}\")\n",
    "print(f\"验证: 6×{x.item()} = {6*x.item()} ✓\")\n",
    "print()\n",
    "\n",
    "# 3. torch.autograd.grad vs .backward()\n",
    "print(\"🔧 torch.autograd.grad vs .backward():\")\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# 方法1：使用 .backward()（修改 x.grad）\n",
    "y.backward()\n",
    "print(f\"方法1 (.backward): x.grad = {x.grad}\")\n",
    "\n",
    "# 方法2：使用 torch.autograd.grad（返回梯度值）\n",
    "x.grad = None  # 清空梯度\n",
    "y = x ** 2  # 重新计算（需要重新构建图）\n",
    "grad_value = torch.autograd.grad(y, x)[0]\n",
    "print(f\"方法2 (autograd.grad): 返回值 = {grad_value}\")\n",
    "print(f\"方法2 不修改 x.grad: {x.grad}\")\n",
    "\n",
    "print(\"\\n💡 两种方法的区别:\")\n",
    "print(\"- .backward(): 将梯度存储在 .grad 属性中\")\n",
    "print(\"- autograd.grad(): 返回梯度值，不修改 .grad 属性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 4. 实际应用：线性回归的梯度下降\n",
    "\n",
    "让我们用自动求导来实现一个完整的线性回归训练过程！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 实战：用自动求导实现线性回归\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. 生成模拟数据\n",
    "# 真实关系：y = 2x + 1 + noise\n",
    "torch.manual_seed(42)\n",
    "n_samples = 100\n",
    "true_w = 2.0  # 真实斜率\n",
    "true_b = 1.0  # 真实截距\n",
    "\n",
    "# 生成数据\n",
    "X = torch.randn(n_samples, 1)  # 输入特征\n",
    "noise = torch.randn(n_samples, 1) * 0.3  # 噪声\n",
    "y_true = true_w * X + true_b + noise  # 真实标签\n",
    "\n",
    "print(f\"📊 数据生成完成:\")\n",
    "print(f\"样本数量: {n_samples}\")\n",
    "print(f\"真实参数: w = {true_w}, b = {true_b}\")\n",
    "print(f\"输入范围: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"输出范围: [{y_true.min():.2f}, {y_true.max():.2f}]\")\n",
    "print()\n",
    "\n",
    "# 2. 初始化模型参数（需要梯度！）\n",
    "w = torch.randn(1, 1, requires_grad=True)  # 权重\n",
    "b = torch.randn(1, requires_grad=True)     # 偏置\n",
    "\n",
    "print(f\"🎲 初始参数:\")\n",
    "print(f\"初始 w = {w.item():.4f}\")\n",
    "print(f\"初始 b = {b.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# 3. 定义前向传播和损失函数\n",
    "def forward(X, w, b):\n",
    "    \"\"\"线性回归前向传播\"\"\"\n",
    "    return X @ w + b  # 矩阵乘法 + 广播\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"均方误差损失函数\"\"\"\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "# 4. 训练循环\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "losses = []  # 记录损失值\n",
    "\n",
    "print(f\"🚀 开始训练:\")\n",
    "print(f\"学习率: {learning_rate}\")\n",
    "print(f\"训练轮数: {num_epochs}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    y_pred = forward(X, w, b)\n",
    "    loss = mse_loss(y_pred, y_true)\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()  # 计算梯度\n",
    "    \n",
    "    # 参数更新（梯度下降）\n",
    "    with torch.no_grad():  # 更新参数时不需要计算梯度\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # 清零梯度（重要！）\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # 记录损失\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 打印进度\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.6f}, w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(f\"✅ 训练完成!\")\n",
    "print(f\"最终参数: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print(f\"真实参数: w = {true_w:.4f}, b = {true_b:.4f}\")\n",
    "print(f\"参数误差: w误差 = {abs(w.item() - true_w):.4f}, b误差 = {abs(b.item() - true_b):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 损失曲线\n",
    "ax1.plot(losses, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('训练损失曲线')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 数据和拟合结果\n",
    "with torch.no_grad():\n",
    "    # 创建测试数据用于绘制直线\n",
    "    x_test = torch.linspace(X.min(), X.max(), 100).unsqueeze(1)\n",
    "    y_pred_test = forward(x_test, w, b)\n",
    "    y_true_test = true_w * x_test + true_b\n",
    "\n",
    "# 散点图和拟合直线\n",
    "ax2.scatter(X.numpy(), y_true.numpy(), alpha=0.6, label='真实数据', s=20)\n",
    "ax2.plot(x_test.numpy(), y_pred_test.numpy(), 'r-', \n",
    "         label=f'学习到的直线 (w={w.item():.2f}, b={b.item():.2f})', linewidth=2)\n",
    "ax2.plot(x_test.numpy(), y_true_test.numpy(), 'g--', \n",
    "         label=f'真实直线 (w={true_w:.2f}, b={true_b:.2f})', linewidth=2)\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('线性回归拟合结果')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🎉 太棒了！自动求导帮我们完美地学习到了数据的潜在规律！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚨 5. 常见陷阱和注意事项\n",
    "\n",
    "在使用自动求导时，有一些常见的陷阱需要注意："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚨 常见陷阱和解决方案\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 陷阱1：忘记清零梯度\n",
    "print(\"⚠️ 陷阱1: 忘记清零梯度\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    print(f\"第{i+1}次: x.grad = {x.grad}  # 梯度累积了！\")\n",
    "    # 正确做法：每次都要清零\n",
    "    # x.grad.zero_()  # 取消注释以修复\n",
    "\n",
    "print(\"💡 解决方案: 每次backward()前调用 x.grad.zero_()\\n\")\n",
    "\n",
    "# 陷阱2：试图对非标量进行backward\n",
    "print(\"⚠️ 陷阱2: 对非标量进行backward\")\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = x ** 2  # 结果是向量\n",
    "try:\n",
    "    y.backward()  # 这会出错！\n",
    "except RuntimeError as e:\n",
    "    print(f\"错误: {str(e)[:50]}...\")\n",
    "    print(\"💡 解决方案1: 先求和再backward\")\n",
    "    y.sum().backward()\n",
    "    print(f\"求和后backward成功: x.grad = {x.grad}\")\n",
    "    \n",
    "# 或者提供gradient参数\n",
    "x.grad.zero_()\n",
    "y = x ** 2\n",
    "gradient_weights = torch.tensor([1.0, 1.0])\n",
    "y.backward(gradient_weights)\n",
    "print(f\"💡 解决方案2: 提供gradient参数: x.grad = {x.grad}\\n\")\n",
    "\n",
    "# 陷阱3：in-place操作破坏计算图\n",
    "print(\"⚠️ 陷阱3: in-place操作破坏计算图\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "try:\n",
    "    y += 1  # in-place操作，等价于 y.add_(1)\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"错误: in-place操作可能破坏计算图\")\n",
    "    print(\"💡 解决方案: 使用非in-place操作\")\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x ** 2\n",
    "    z = y + 1  # 非in-place操作\n",
    "    z.backward()\n",
    "    print(f\"修复后: x.grad = {x.grad}\\n\")\n",
    "\n",
    "# 陷阱4：在no_grad上下文中意外关闭梯度\n",
    "print(\"⚠️ 陷阱4: 意外关闭梯度计算\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x ** 2  # 在no_grad中计算\n",
    "print(f\"y.requires_grad = {y.requires_grad}  # False！梯度被关闭了\")\n",
    "print(\"💡 解决方案: 确保需要梯度的计算在no_grad之外\\n\")\n",
    "\n",
    "# 陷阱5：重复使用中间变量\n",
    "print(\"⚠️ 陷阱5: 重复backward同一个图\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()  # 第一次backward\n",
    "try:\n",
    "    y.backward()  # 第二次backward，图已经被释放！\n",
    "except RuntimeError as e:\n",
    "    print(f\"错误: 计算图已被释放\")\n",
    "    print(\"💡 解决方案: 使用 retain_graph=True 或重新计算\")\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x ** 2\n",
    "    y.backward(retain_graph=True)  # 保留图\n",
    "    x.grad.zero_()  # 清零梯度\n",
    "    y.backward()  # 现在可以再次backward\n",
    "    print(f\"修复后: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 6. 自动求导的高级应用\n",
    "\n",
    "让我们探索一些高级应用场景："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 高级应用场景\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. 计算雅可比矩阵 (Jacobian)\n",
    "print(\"📊 计算雅可比矩阵:\")\n",
    "\n",
    "def compute_jacobian(func, inputs):\n",
    "    \"\"\"计算函数的雅可比矩阵\"\"\"\n",
    "    outputs = func(inputs)\n",
    "    jacobian = torch.zeros(outputs.size(0), inputs.size(0))\n",
    "    \n",
    "    for i in range(outputs.size(0)):\n",
    "        # 为每个输出分量计算梯度\n",
    "        grad_outputs = torch.zeros_like(outputs)\n",
    "        grad_outputs[i] = 1.0\n",
    "        \n",
    "        inputs.grad = None  # 清零梯度\n",
    "        grad_inputs = torch.autograd.grad(outputs, inputs, \n",
    "                                        grad_outputs=grad_outputs,\n",
    "                                        retain_graph=True)[0]\n",
    "        jacobian[i] = grad_inputs\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# 示例：f(x) = [x1², x1*x2, x2²]\n",
    "def vector_function(x):\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return torch.stack([x1**2, x1*x2, x2**2])\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "jacobian = compute_jacobian(vector_function, x)\n",
    "\n",
    "print(f\"输入: x = {x.data}\")\n",
    "print(f\"函数: f(x) = [x1², x1*x2, x2²]\")\n",
    "print(f\"雅可比矩阵:\\n{jacobian}\")\n",
    "print(f\"理论值:\")\n",
    "print(f\"df1/dx1 = 2*x1 = {2*x[0]}, df1/dx2 = 0\")\n",
    "print(f\"df2/dx1 = x2 = {x[1]}, df2/dx2 = x1 = {x[0]}\")\n",
    "print(f\"df3/dx1 = 0, df3/dx2 = 2*x2 = {2*x[1]}\\n\")\n",
    "\n",
    "# 2. 梯度检查 (Gradient Checking)\n",
    "print(\"🔍 梯度检查:\")\n",
    "\n",
    "def numerical_gradient(func, x, eps=1e-7):\n",
    "    \"\"\"数值方法计算梯度\"\"\"\n",
    "    grad = torch.zeros_like(x)\n",
    "    for i in range(x.size(0)):\n",
    "        x_plus = x.clone()\n",
    "        x_minus = x.clone()\n",
    "        x_plus[i] += eps\n",
    "        x_minus[i] -= eps\n",
    "        \n",
    "        grad[i] = (func(x_plus) - func(x_minus)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "def test_function(x):\n",
    "    return (x ** 3).sum()  # f(x) = x1³ + x2³\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# 自动求导\n",
    "y = test_function(x)\n",
    "y.backward()\n",
    "auto_grad = x.grad.clone()\n",
    "\n",
    "# 数值求导\n",
    "x_no_grad = torch.tensor([2.0, 3.0])  # 不需要梯度的版本\n",
    "numerical_grad = numerical_gradient(test_function, x_no_grad)\n",
    "\n",
    "print(f\"自动求导梯度: {auto_grad}\")\n",
    "print(f\"数值求导梯度: {numerical_grad}\")\n",
    "print(f\"差异: {torch.abs(auto_grad - numerical_grad)}\")\n",
    "print(f\"相对误差: {torch.abs(auto_grad - numerical_grad) / torch.abs(auto_grad)}\")\n",
    "print(\"✅ 梯度检查通过！(差异极小)\\n\")\n",
    "\n",
    "# 3. 条件梯度计算\n",
    "print(\"🎭 条件梯度计算:\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "condition = x > 1.0\n",
    "\n",
    "if condition:\n",
    "    y = x ** 2\n",
    "else:\n",
    "    y = x ** 3\n",
    "\n",
    "y.backward()\n",
    "print(f\"当 x = {x.item()} > 1 时:\")\n",
    "print(f\"使用函数 y = x²\")\n",
    "print(f\"梯度 dy/dx = 2x = {x.grad}\")\n",
    "\n",
    "# 测试另一个条件\n",
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "if x > 1.0:\n",
    "    y = x ** 2\n",
    "else:\n",
    "    y = x ** 3\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\n当 x = {x.item()} ≤ 1 时:\")\n",
    "print(f\"使用函数 y = x³\")\n",
    "print(f\"梯度 dy/dx = 3x² = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 7. 实战：多层感知机的反向传播\n",
    "\n",
    "让我们手动实现一个简单的神经网络，深入理解反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧠 实战：手动实现神经网络反向传播\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"简单的两层神经网络\n",
    "        \n",
    "        网络结构: input → hidden → output\n",
    "        激活函数: ReLU (隐藏层), 无激活 (输出层)\n",
    "        \"\"\"\n",
    "        # 初始化权重（需要梯度！）\n",
    "        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True) * 0.5\n",
    "        self.b1 = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        self.W2 = torch.randn(hidden_size, output_size, requires_grad=True) * 0.5\n",
    "        self.b2 = torch.zeros(output_size, requires_grad=True)\n",
    "        \n",
    "        print(f\"🏗️ 网络结构: {input_size} → {hidden_size} → {output_size}\")\n",
    "        print(f\"参数数量: {self.count_parameters()}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 第一层: input → hidden\n",
    "        z1 = x @ self.W1 + self.b1  # 线性变换\n",
    "        a1 = torch.relu(z1)         # ReLU激活\n",
    "        \n",
    "        # 第二层: hidden → output\n",
    "        z2 = a1 @ self.W2 + self.b2  # 线性变换\n",
    "        \n",
    "        return z2\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"统计参数数量\"\"\"\n",
    "        total = 0\n",
    "        for param in [self.W1, self.b1, self.W2, self.b2]:\n",
    "            total += param.numel()\n",
    "        return total\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"清零所有参数的梯度\"\"\"\n",
    "        for param in [self.W1, self.b1, self.W2, self.b2]:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有参数\"\"\"\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "# 创建网络和数据\n",
    "torch.manual_seed(42)\n",
    "net = SimpleNeuralNetwork(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# 生成一批样本数据\n",
    "batch_size = 4\n",
    "X = torch.randn(batch_size, 3)  # 4个样本，每个3维特征\n",
    "y_true = torch.randn(batch_size, 2)  # 4个样本，每个2维输出\n",
    "\n",
    "print(f\"\\n📊 数据信息:\")\n",
    "print(f\"输入形状: {X.shape}\")\n",
    "print(f\"目标形状: {y_true.shape}\")\n",
    "print()\n",
    "\n",
    "# 训练一个step，详细观察梯度\n",
    "print(f\"🔄 详细的一步训练过程:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. 前向传播\n",
    "print(\"1️⃣ 前向传播:\")\n",
    "y_pred = net.forward(X)\n",
    "print(f\"预测输出形状: {y_pred.shape}\")\n",
    "print(f\"预测值范围: [{y_pred.min():.3f}, {y_pred.max():.3f}]\")\n",
    "\n",
    "# 2. 计算损失\n",
    "print(\"\\n2️⃣ 计算损失:\")\n",
    "loss = torch.nn.functional.mse_loss(y_pred, y_true)\n",
    "print(f\"MSE损失: {loss.item():.6f}\")\n",
    "\n",
    "# 3. 反向传播\n",
    "print(\"\\n3️⃣ 反向传播:\")\n",
    "print(\"计算所有参数的梯度...\")\n",
    "loss.backward()\n",
    "\n",
    "# 检查梯度\n",
    "print(\"\\n📊 梯度统计:\")\n",
    "param_names = ['W1', 'b1', 'W2', 'b2']\n",
    "for name, param in zip(param_names, net.get_parameters()):\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.norm().item()\n",
    "        grad_mean = param.grad.mean().item()\n",
    "        print(f\"{name}: 梯度范数={grad_norm:.6f}, 梯度均值={grad_mean:.6f}\")\n",
    "    else:\n",
    "        print(f\"{name}: 梯度为None\")\n",
    "\n",
    "# 4. 参数更新\n",
    "print(\"\\n4️⃣ 参数更新:\")\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    for param in net.get_parameters():\n",
    "        param -= learning_rate * param.grad\n",
    "print(f\"学习率: {learning_rate}\")\n",
    "print(\"参数更新完成\")\n",
    "\n",
    "# 5. 清零梯度\n",
    "print(\"\\n5️⃣ 清零梯度:\")\n",
    "net.zero_grad()\n",
    "print(\"梯度清零完成\")\n",
    "\n",
    "print(\"\\n🎉 一步完整的训练循环完成！\")\n",
    "print(\"这就是深度学习训练的核心过程：前向→损失→反向→更新→清零\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 可视化梯度流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_flow(net, X, y_true, num_steps=10):\n",
    "    \"\"\"可视化训练过程中的梯度变化\"\"\"\n",
    "    losses = []\n",
    "    w1_grads = []\n",
    "    w2_grads = []\n",
    "    \n",
    "    print(\"📈 梯度流可视化 (前10步训练)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # 前向传播\n",
    "        y_pred = net.forward(X)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y_true)\n",
    "        \n",
    "        # 反向传播\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 记录数据\n",
    "        losses.append(loss.item())\n",
    "        w1_grads.append(net.W1.grad.norm().item())\n",
    "        w2_grads.append(net.W2.grad.norm().item())\n",
    "        \n",
    "        # 参数更新\n",
    "        with torch.no_grad():\n",
    "            for param in net.get_parameters():\n",
    "                param -= 0.01 * param.grad\n",
    "        \n",
    "        if step % 3 == 0:\n",
    "            print(f\"Step {step:2d}: Loss={loss.item():.4f}, \"\n",
    "                  f\"W1_grad_norm={w1_grads[-1]:.4f}, \"\n",
    "                  f\"W2_grad_norm={w2_grads[-1]:.4f}\")\n",
    "    \n",
    "    # 绘制图像\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # 损失曲线\n",
    "    ax1.plot(losses, 'b-', marker='o', markersize=4)\n",
    "    ax1.set_xlabel('Training Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('训练损失变化')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 梯度范数\n",
    "    ax2.plot(w1_grads, 'r-', marker='s', markersize=4, label='W1 梯度范数')\n",
    "    ax2.plot(w2_grads, 'g-', marker='^', markersize=4, label='W2 梯度范数')\n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Gradient Norm')\n",
    "    ax2.set_title('梯度范数变化')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return losses, w1_grads, w2_grads\n",
    "\n",
    "# 运行可视化\n",
    "losses, w1_grads, w2_grads = visualize_gradient_flow(net, X, y_true)\n",
    "\n",
    "print(f\"\\n📊 观察结果:\")\n",
    "print(f\"• 损失从 {losses[0]:.4f} 降至 {losses[-1]:.4f}\")\n",
    "print(f\"• W1层梯度范数: {w1_grads[0]:.4f} → {w1_grads[-1]:.4f}\")\n",
    "print(f\"• W2层梯度范数: {w2_grads[0]:.4f} → {w2_grads[-1]:.4f}\")\n",
    "print(f\"✅ 梯度正常流动，网络在学习！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 8. 自动求导原理深度解析\n",
    "\n",
    "让我们深入理解自动求导的底层原理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔬 自动求导原理深度解析\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. 链式法则的逐步展示\n",
    "print(\"⛓️ 链式法则详细展示:\")\n",
    "print(\"函数: f(x) = sin(x²)\")\n",
    "print(\"分解: u = x², v = sin(u), f = v\")\n",
    "print(\"链式法则: df/dx = (df/dv) × (dv/du) × (du/dx)\")\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# 逐步计算，保存中间结果\n",
    "u = x ** 2\n",
    "print(f\"\\n中间步骤:\")\n",
    "print(f\"x = {x.item():.4f}\")\n",
    "print(f\"u = x² = {u.item():.4f}\")\n",
    "\n",
    "v = torch.sin(u)\n",
    "print(f\"v = sin(u) = {v.item():.4f}\")\n",
    "\n",
    "# 反向传播\n",
    "v.backward()\n",
    "print(f\"\\n梯度计算:\")\n",
    "print(f\"df/dx = {x.grad:.6f}\")\n",
    "\n",
    "# 手动验证\n",
    "manual_grad = torch.cos(u) * 2 * x\n",
    "print(f\"手动计算: cos({u.item():.4f}) × 2 × {x.item()} = {manual_grad.item():.6f}\")\n",
    "print(f\"误差: {abs(x.grad.item() - manual_grad.item()):.10f}\\n\")\n",
    "\n",
    "# 2. 不同运算的局部梯度\n",
    "print(\"🧮 常见运算的局部梯度:\")\n",
    "operations = [\n",
    "    (\"加法\", \"z = x + y\", \"dz/dx = 1, dz/dy = 1\"),\n",
    "    (\"乘法\", \"z = x * y\", \"dz/dx = y, dz/dy = x\"),\n",
    "    (\"指数\", \"z = x^n\", \"dz/dx = n * x^(n-1)\"),\n",
    "    (\"对数\", \"z = log(x)\", \"dz/dx = 1/x\"),\n",
    "    (\"ReLU\", \"z = max(0,x)\", \"dz/dx = 1 if x>0 else 0\"),\n",
    "    (\"Sigmoid\", \"z = 1/(1+e^(-x))\", \"dz/dx = z*(1-z)\")\n",
    "]\n",
    "\n",
    "for name, func, grad in operations:\n",
    "    print(f\"{name:8s}: {func:15s} → {grad}\")\n",
    "\n",
    "print(\"\\n💡 自动求导就是自动应用这些规则！\")\n",
    "\n",
    "# 3. 计算图的内存管理\n",
    "print(\"\\n💾 计算图的内存管理:\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y * 3\n",
    "\n",
    "print(f\"创建计算图: x → y → z\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")\n",
    "print(f\"x.grad_fn: {x.grad_fn}\")\n",
    "\n",
    "# 检查引用计数\n",
    "import sys\n",
    "print(f\"\\nPython引用计数:\")\n",
    "print(f\"x引用数: {sys.getrefcount(x)}\")\n",
    "print(f\"y引用数: {sys.getrefcount(y)}\")\n",
    "print(f\"z引用数: {sys.getrefcount(z)}\")\n",
    "\n",
    "# backward后图被释放\n",
    "z.backward()\n",
    "print(f\"\\nbackward后:\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}  # 图被释放\")\n",
    "\n",
    "# 4. 动态计算图 vs 静态计算图\n",
    "print(\"\\n🔄 动态计算图的优势:\")\n",
    "\n",
    "for i in range(3):\n",
    "    x = torch.tensor(float(i+1), requires_grad=True)\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        y = x ** 2  # 偶数次迭代\n",
    "    else:\n",
    "        y = x ** 3  # 奇数次迭代\n",
    "    \n",
    "    y.backward()\n",
    "    print(f\"迭代{i}: x={x.item()}, 函数=x^{2 if i%2==0 else 3}, 梯度={x.grad.item()}\")\n",
    "\n",
    "print(\"💡 每次前向传播都可以使用不同的计算图！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 总结与关键要点\n",
    "\n",
    "### 🎯 核心概念回顾\n",
    "\n",
    "1. **自动求导是深度学习的基石** - 让我们无需手动推导复杂的梯度公式\n",
    "2. **计算图记录运算历史** - 前向传播构建图，反向传播计算梯度\n",
    "3. **链式法则自动应用** - PyTorch自动处理复合函数的梯度计算\n",
    "4. **动态图的灵活性** - 每次前向传播都可以使用不同的计算逻辑\n",
    "\n",
    "### 💡 实用技巧总结\n",
    "\n",
    "- ✅ **始终记住清零梯度**: `tensor.grad.zero_()` 或 `tensor.grad = None`\n",
    "- ✅ **合理使用 `requires_grad`**: 只为需要梯度的张量设置\n",
    "- ✅ **避免 in-place 操作**: 使用 `y = x + 1` 而不是 `x += 1`\n",
    "- ✅ **理解 `no_grad` 上下文**: 推理时关闭梯度计算节省内存\n",
    "- ✅ **使用梯度检查验证**: 数值梯度 vs 自动梯度\n",
    "\n",
    "### 🚀 下一步学习建议\n",
    "\n",
    "1. **优化器 (Optimizers)** - SGD, Adam, RMSprop等\n",
    "2. **损失函数 (Loss Functions)** - 各种损失函数的设计原理\n",
    "3. **神经网络模块 (nn.Module)** - 构建复杂模型的基础\n",
    "4. **高级自动求导** - 高阶导数、雅可比矩阵等\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏆 恭喜完成自动求导机制学习！\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 最终测试：综合应用\n",
    "print(\"🎯 综合测试: 实现一个简单的优化问题\")\n",
    "print(\"问题: 找到函数 f(x,y) = (x-3)² + (y-2)² 的最小值\")\n",
    "print(\"理论答案: x=3, y=2时达到最小值0\")\n",
    "\n",
    "# 使用梯度下降求解\n",
    "x = torch.tensor(0.0, requires_grad=True)  # 起始点 (0, 0)\n",
    "y = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(50):\n",
    "    # 目标函数\n",
    "    f = (x - 3)**2 + (y - 2)**2\n",
    "    \n",
    "    # 反向传播\n",
    "    f.backward()\n",
    "    \n",
    "    # 梯度下降更新\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "        y -= learning_rate * y.grad\n",
    "    \n",
    "    # 清零梯度\n",
    "    x.grad.zero_()\n",
    "    y.grad.zero_()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:2d}: f={f.item():.6f}, x={x.item():.4f}, y={y.item():.4f}\")\n",
    "\n",
    "print(f\"\\n🎉 最终结果:\")\n",
    "print(f\"找到的最优解: x = {x.item():.4f}, y = {y.item():.4f}\")\n",
    "print(f\"理论最优解:   x = 3.0000, y = 2.0000\")\n",
    "print(f\"最小函数值:   f = {f.item():.8f}\")\n",
    "\n",
    "error = ((x.item()-3)**2 + (y.item()-2)**2)**0.5\n",
    "print(f\"误差距离:     {error:.6f}\")\n",
    "\n",
    "if error < 0.01:\n",
    "    print(\"✅ 优化成功！误差在可接受范围内\")\n",
    "else:\n",
    "    print(\"❌ 需要更多训练步数或调整学习率\")\n",
    "\n",
    "print(\"\\n💪 你已经掌握了自动求导的核心原理和应用！\")\n",
    "print(\"现在可以开始学习更高级的深度学习概念了。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}