{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”„ PyTorch è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ (Autograd) æ·±åº¦è§£æ\n",
    "\n",
    "## ğŸ“š æœ¬èŠ‚å­¦ä¹ ç›®æ ‡\n",
    "- æ·±å…¥ç†è§£è‡ªåŠ¨æ±‚å¯¼çš„åŸç†å’Œé‡è¦æ€§\n",
    "- æŒæ¡è®¡ç®—å›¾çš„æ¦‚å¿µå’Œæ„å»ºè¿‡ç¨‹\n",
    "- ç†Ÿç»ƒä½¿ç”¨ `requires_grad` å’Œ `backward()`\n",
    "- ç†è§£æ¢¯åº¦çš„è®¡ç®—ã€å­˜å‚¨å’Œæ¸…é›¶\n",
    "- æŒæ¡é“¾å¼æ³•åˆ™åœ¨è‡ªåŠ¨æ±‚å¯¼ä¸­çš„åº”ç”¨\n",
    "- å­¦ä¼šå¤„ç†å¤æ‚çš„æ¢¯åº¦è®¡ç®—åœºæ™¯\n",
    "- äº†è§£è‡ªåŠ¨æ±‚å¯¼çš„é«˜çº§ç‰¹æ€§å’Œæ³¨æ„äº‹é¡¹\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦è‡ªåŠ¨æ±‚å¯¼ï¼Ÿ\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦æ—¢å®¹æ˜“å‡ºé”™åˆæå…¶å¤æ‚ï¼Œå°¤å…¶æ˜¯åœ¨æ·±å±‚ç¥ç»ç½‘ç»œä¸­ã€‚\n",
    "\n",
    "**è‡ªåŠ¨æ±‚å¯¼çš„ä¼˜åŠ¿ï¼š**\n",
    "- ğŸ¯ **è‡ªåŠ¨åŒ–**ï¼šæ— éœ€æ‰‹åŠ¨æ¨å¯¼å¤æ‚çš„æ¢¯åº¦å…¬å¼\n",
    "- âœ… **å‡†ç¡®æ€§**ï¼šé¿å…äººä¸ºè®¡ç®—é”™è¯¯\n",
    "- ğŸš€ **æ•ˆç‡**ï¼šä¼˜åŒ–çš„è®¡ç®—å›¾æ‰§è¡Œ\n",
    "- ğŸ”§ **çµæ´»æ€§**ï¼šæ”¯æŒåŠ¨æ€è®¡ç®—å›¾\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from utils.common import get_device, print_tensor_info, set_seed\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"ğŸ”¥ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "device = get_device()\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” 1. è‡ªåŠ¨æ±‚å¯¼åŸºç¡€æ¦‚å¿µ\n",
    "\n",
    "### 1.1 ä»€ä¹ˆæ˜¯è®¡ç®—å›¾ï¼Ÿ\n",
    "\n",
    "**è®¡ç®—å›¾(Computational Graph)** æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼Œç”¨æ¥è¡¨ç¤ºæ•°å­¦è¿ç®—çš„æ‰§è¡Œè¿‡ç¨‹ï¼š\n",
    "- **èŠ‚ç‚¹**ï¼šè¡¨ç¤ºå˜é‡æˆ–è¿ç®—\n",
    "- **è¾¹**ï¼šè¡¨ç¤ºæ•°æ®æµå‘\n",
    "- **å‰å‘ä¼ æ’­**ï¼šä»è¾“å…¥åˆ°è¾“å‡ºè®¡ç®—ç»“æœ\n",
    "- **åå‘ä¼ æ’­**ï¼šä»è¾“å‡ºåˆ°è¾“å…¥è®¡ç®—æ¢¯åº¦\n",
    "\n",
    "### ğŸŒ° ä¸¾ä¸ªç”Ÿæ´»åŒ–çš„ä¾‹å­\n",
    "å‡è®¾ä½ åœ¨åšèœï¼Œè®¡ç®—æ€»æˆæœ¬ï¼š\n",
    "```\n",
    "è”¬èœä»·æ ¼(a) = 10å…ƒ    è‚‰ä»·æ ¼(b) = 20å…ƒ\n",
    "    â†“                   â†“\n",
    "  è”¬èœæˆæœ¬ = a Ã— 2    è‚‰æˆæœ¬ = b Ã— 1.5\n",
    "    â†“                   â†“\n",
    "      æ€»æˆæœ¬(c) = è”¬èœæˆæœ¬ + è‚‰æˆæœ¬\n",
    "```\n",
    "\n",
    "å¦‚æœè”¬èœä»·æ ¼å˜åŒ–1å…ƒï¼Œæ€»æˆæœ¬å˜åŒ–å¤šå°‘ï¼Ÿè¿™å°±æ˜¯æ±‚æ¢¯åº¦ âˆ‚c/âˆ‚aï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” è‡ªåŠ¨æ±‚å¯¼åŸºç¡€æ¼”ç¤º\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡ - è¿™æ˜¯å…³é”®ï¼\n",
    "# requires_grad=True å‘Šè¯‰PyTorchï¼š\"æˆ‘éœ€è¦è®¡ç®—è¿™ä¸ªå¼ é‡çš„æ¢¯åº¦\"\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "print(f\"ğŸ“Š è¾“å…¥å˜é‡:\")\n",
    "print(f\"x = {x}, requires_grad = {x.requires_grad}\")\n",
    "print(f\"y = {y}, requires_grad = {y.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªç®€å•çš„è®¡ç®—ï¼šz = xÂ² + 2xy + yÂ²\n",
    "# è¿™ç›¸å½“äº z = (x + y)Â²\n",
    "z = x**2 + 2*x*y + y**2\n",
    "\n",
    "print(f\"ğŸ§® è®¡ç®—è¿‡ç¨‹:\")\n",
    "print(f\"z = xÂ² + 2xy + yÂ² = {x}Â² + 2Ã—{x}Ã—{y} + {y}Â² = {z}\")\n",
    "print(f\"z.requires_grad = {z.requires_grad}  # è‡ªåŠ¨ç»§æ‰¿\")\n",
    "print(f\"z.grad_fn = {z.grad_fn}  # è®°å½•å¦‚ä½•è®¡ç®—å¾—åˆ°z\")\n",
    "print()\n",
    "\n",
    "# è®¡ç®—æ¢¯åº¦ - åå‘ä¼ æ’­ï¼\n",
    "print(f\"ğŸ”„ å¼€å§‹åå‘ä¼ æ’­...\")\n",
    "z.backward()  # è®¡ç®— dz/dx å’Œ dz/dy\n",
    "\n",
    "print(f\"âœ… æ¢¯åº¦è®¡ç®—å®Œæˆ!\")\n",
    "print(f\"dz/dx = {x.grad}  # ç†è®ºå€¼: 2x + 2y = 2Ã—{x.item()} + 2Ã—{y.item()} = {2*x.item() + 2*y.item()}\")\n",
    "print(f\"dz/dy = {y.grad}  # ç†è®ºå€¼: 2y + 2x = 2Ã—{y.item()} + 2Ã—{x.item()} = {2*y.item() + 2*x.item()}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ è§‚å¯Ÿï¼šè‡ªåŠ¨æ±‚å¯¼çš„ç»“æœä¸ç†è®ºè®¡ç®—å®Œå…¨ä¸€è‡´ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 requires_grad çš„ä½œç”¨æœºåˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ requires_grad è¯¦è§£\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. é»˜è®¤æƒ…å†µï¼šä¸éœ€è¦æ¢¯åº¦\n",
    "a = torch.tensor(5.0)  # requires_grad=False (é»˜è®¤)\n",
    "b = torch.tensor(3.0, requires_grad=True)  # æ˜¾å¼è®¾ç½®ä¸ºTrue\n",
    "\n",
    "print(f\"ğŸ“Š åŸºç¡€å¼ é‡:\")\n",
    "print(f\"a = {a}, requires_grad = {a.requires_grad}\")\n",
    "print(f\"b = {b}, requires_grad = {b.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# 2. ä¼ æ’­è§„åˆ™ï¼šåªè¦æœ‰ä¸€ä¸ªæ“ä½œæ•°éœ€è¦æ¢¯åº¦ï¼Œç»“æœå°±éœ€è¦æ¢¯åº¦\n",
    "c = a + b  # aä¸éœ€è¦æ¢¯åº¦ï¼Œbéœ€è¦æ¢¯åº¦ â†’ céœ€è¦æ¢¯åº¦\n",
    "d = a * 2  # aä¸éœ€è¦æ¢¯åº¦ï¼Œå¸¸æ•°è¿ç®— â†’ dä¸éœ€è¦æ¢¯åº¦\n",
    "\n",
    "print(f\"ğŸ”„ ä¼ æ’­è§„åˆ™æ¼”ç¤º:\")\n",
    "print(f\"c = a + b = {c}, requires_grad = {c.requires_grad}  # ç»§æ‰¿è‡ªb\")\n",
    "print(f\"d = a * 2 = {d}, requires_grad = {d.requires_grad}  # aä¸éœ€è¦æ¢¯åº¦\")\n",
    "print()\n",
    "\n",
    "# 3. åŠ¨æ€æ§åˆ¶ï¼šå¯ä»¥éšæ—¶å¼€å¯/å…³é—­æ¢¯åº¦è®¡ç®—\n",
    "print(f\"ğŸ›ï¸ åŠ¨æ€æ§åˆ¶æ¢¯åº¦:\")\n",
    "\n",
    "# å¼€å¯æ¢¯åº¦\n",
    "a.requires_grad_(True)  # æ³¨æ„ï¼šå¸¦ä¸‹åˆ’çº¿çš„æ–¹æ³•æ˜¯å°±åœ°æ“ä½œ\n",
    "print(f\"å¼€å¯å a.requires_grad = {a.requires_grad}\")\n",
    "\n",
    "# å…³é—­æ¢¯åº¦\n",
    "b.requires_grad_(False)\n",
    "print(f\"å…³é—­å b.requires_grad = {b.requires_grad}\")\n",
    "\n",
    "# 4. åªæœ‰æµ®ç‚¹æ•°ç±»å‹æ‰èƒ½è®¡ç®—æ¢¯åº¦\n",
    "try:\n",
    "    int_tensor = torch.tensor(5, requires_grad=True)  # æ•´æ•°ç±»å‹\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nâŒ é”™è¯¯: {e}\")\n",
    "    print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ .float() è½¬æ¢ä¸ºæµ®ç‚¹ç±»å‹\")\n",
    "    int_tensor = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "    print(f\"âœ… ä¿®æ­£å: {int_tensor}, dtype = {int_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  2. è®¡ç®—å›¾çš„æ„å»ºå’Œæ‰§è¡Œ\n",
    "\n",
    "PyTorchä½¿ç”¨**åŠ¨æ€è®¡ç®—å›¾**ï¼Œè¿™æ„å‘³ç€å›¾æ˜¯åœ¨è¿è¡Œæ—¶æ„å»ºçš„ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­éƒ½ä¼šé‡æ–°æ„å»ºå›¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§  è®¡ç®—å›¾æ„å»ºè¿‡ç¨‹è¯¦è§£\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åˆ›å»ºè¾“å…¥èŠ‚ç‚¹\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"ğŸ¯ æ­¥éª¤1: åˆ›å»ºè¾“å…¥èŠ‚ç‚¹\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.grad_fn = {x.grad_fn}  # è¾“å…¥èŠ‚ç‚¹æ²¡æœ‰grad_fn\")\n",
    "print()\n",
    "\n",
    "# é€æ­¥æ„å»ºè®¡ç®—å›¾\n",
    "print(f\"ğŸ”¨ æ­¥éª¤2: é€æ­¥æ„å»ºè®¡ç®—å›¾\")\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šxÂ² \n",
    "x_squared = x ** 2\n",
    "print(f\"y1 = xÂ² = {x_squared}\")\n",
    "print(f\"y1.grad_fn = {x_squared.grad_fn}  # è®°å½•è¿™æ˜¯å¹‚è¿ç®—\")\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼š2xÂ²\n",
    "two_x_squared = 2 * x_squared \n",
    "print(f\"y2 = 2Ã—y1 = {two_x_squared}\")\n",
    "print(f\"y2.grad_fn = {two_x_squared.grad_fn}  # è®°å½•è¿™æ˜¯ä¹˜æ³•è¿ç®—\")\n",
    "\n",
    "# ç¬¬ä¸‰æ­¥ï¼š2xÂ² + 1\n",
    "final_result = two_x_squared + 1\n",
    "print(f\"y3 = y2 + 1 = {final_result}\")\n",
    "print(f\"y3.grad_fn = {final_result.grad_fn}  # è®°å½•è¿™æ˜¯åŠ æ³•è¿ç®—\")\n",
    "print()\n",
    "\n",
    "# æŸ¥çœ‹è®¡ç®—å›¾çš„next_functions\n",
    "print(f\"ğŸ” è®¡ç®—å›¾è¿æ¥å…³ç³»:\")\n",
    "print(f\"final_result çš„å‰ä¸€ä¸ªå‡½æ•°: {final_result.grad_fn.next_functions}\")\n",
    "print(f\"two_x_squared çš„å‰ä¸€ä¸ªå‡½æ•°: {two_x_squared.grad_fn.next_functions}\")\n",
    "print()\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "print(f\"ğŸ”„ æ­¥éª¤3: åå‘ä¼ æ’­\")\n",
    "print(f\"è®¡ç®— d(2xÂ²+1)/dx åœ¨ x={x.item()} å¤„çš„å€¼\")\n",
    "print(f\"ç†è®ºç»“æœ: d(2xÂ²+1)/dx = 4x = 4Ã—{x.item()} = {4*x.item()}\")\n",
    "\n",
    "final_result.backward()\n",
    "print(f\"è‡ªåŠ¨æ±‚å¯¼ç»“æœ: {x.grad}\")\n",
    "print(f\"âœ… éªŒè¯é€šè¿‡: ç†è®ºå€¼ = è‡ªåŠ¨æ±‚å¯¼å€¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 å¯è§†åŒ–è®¡ç®—å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_computational_graph():\n",
    "    \"\"\"å¯è§†åŒ–ä¸€ä¸ªç®€å•çš„è®¡ç®—å›¾è¿‡ç¨‹\"\"\"\n",
    "    print(\"ğŸ¨ è®¡ç®—å›¾å¯è§†åŒ–\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # é‡æ–°åˆ›å»ºå˜é‡ï¼ˆæ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦ï¼‰\n",
    "    x = torch.tensor(3.0, requires_grad=True)\n",
    "    \n",
    "    print(\"ğŸ“ˆ å‰å‘ä¼ æ’­è¿‡ç¨‹:\")\n",
    "    print(\"\")\n",
    "    print(\"    x = 3.0\")\n",
    "    print(\"    â”‚\")\n",
    "    print(\"    â–¼ (** 2)\")\n",
    "    \n",
    "    y = x ** 2\n",
    "    print(f\"    y = xÂ² = {y.item()}\")\n",
    "    print(\"    â”‚\")\n",
    "    print(\"    â–¼ (* 2)\")\n",
    "    \n",
    "    z = 2 * y\n",
    "    print(f\"    z = 2y = {z.item()}\")\n",
    "    print(\"    â”‚\")\n",
    "    print(\"    â–¼ (+ 1)\")\n",
    "    \n",
    "    w = z + 1\n",
    "    print(f\"    w = z + 1 = {w.item()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"ğŸ“‰ åå‘ä¼ æ’­è¿‡ç¨‹:\")\n",
    "    print(\"\")\n",
    "    w.backward()\n",
    "    \n",
    "    print(\"    dw/dw = 1\")\n",
    "    print(\"    â–²\")\n",
    "    print(\"    â”‚ dw/dz = dw/dw Ã— d(z+1)/dz = 1 Ã— 1 = 1\")\n",
    "    print(\"    â–²\")\n",
    "    print(\"    â”‚ dw/dy = dw/dz Ã— d(2y)/dy = 1 Ã— 2 = 2\")\n",
    "    print(\"    â–²\")\n",
    "    print(\"    â”‚ dw/dx = dw/dy Ã— d(xÂ²)/dx = 2 Ã— 2x = 2 Ã— 2Ã—3 = 12\")\n",
    "    print(f\"    æœ€ç»ˆæ¢¯åº¦: {x.grad}\")\n",
    "    \n",
    "    return x.grad\n",
    "\n",
    "gradient = visualize_computational_graph()\n",
    "print(f\"\\nğŸ’¡ è¿™å°±æ˜¯é“¾å¼æ³•åˆ™çš„è‡ªåŠ¨åº”ç”¨ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ 3. æ¢¯åº¦çš„è®¡ç®—ã€å­˜å‚¨å’Œç®¡ç†\n",
    "\n",
    "ç†è§£æ¢¯åº¦å¦‚ä½•è®¡ç®—ã€å­˜å‚¨å’Œç®¡ç†æ˜¯æŒæ¡è‡ªåŠ¨æ±‚å¯¼çš„å…³é”®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš¡ æ¢¯åº¦ç®¡ç†è¯¦è§£\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. æ¢¯åº¦çš„ç´¯ç§¯ç‰¹æ€§\n",
    "print(\"ğŸ“ˆ æ¢¯åº¦ç´¯ç§¯æ¼”ç¤º:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è®¡ç®—\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"ç¬¬ä¸€æ¬¡: y1 = xÂ², x.grad = {x.grad}\")\n",
    "\n",
    "# ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆä¸æ¸…é›¶æ¢¯åº¦ï¼‰\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"ç¬¬äºŒæ¬¡: y2 = xÂ³, x.grad = {x.grad}  # æ¢¯åº¦ç´¯ç§¯äº†ï¼\")\n",
    "print(f\"è§£é‡Š: ç¬¬ä¸€æ¬¡æ¢¯åº¦(2x=4) + ç¬¬äºŒæ¬¡æ¢¯åº¦(3xÂ²=12) = {4 + 12}\")\n",
    "print()\n",
    "\n",
    "# 2. æ¢¯åº¦æ¸…é›¶ - éå¸¸é‡è¦ï¼\n",
    "print(\"ğŸ§¹ æ¢¯åº¦æ¸…é›¶:\")\n",
    "x.grad.zero_()  # å°±åœ°æ¸…é›¶\n",
    "# æˆ–è€…ä½¿ç”¨: x.grad = None\n",
    "print(f\"æ¸…é›¶å x.grad = {x.grad}\")\n",
    "\n",
    "# é‡æ–°è®¡ç®—\n",
    "y3 = x ** 3\n",
    "y3.backward()\n",
    "print(f\"é‡æ–°è®¡ç®—: y3 = xÂ³, x.grad = {x.grad}  # ç°åœ¨æ˜¯æ­£ç¡®çš„å€¼\")\n",
    "print()\n",
    "\n",
    "# 3. æ ‡é‡vså‘é‡çš„åå‘ä¼ æ’­\n",
    "print(\"ğŸ“Š å‘é‡æ¢¯åº¦è®¡ç®—:\")\n",
    "x_vec = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y_vec = x_vec ** 2  # [1, 4, 9]\n",
    "\n",
    "# å¯¹äºå‘é‡ï¼Œéœ€è¦æä¾›gradientå‚æ•°\n",
    "gradient_weights = torch.tensor([1.0, 1.0, 1.0])  # æ¯ä¸ªå…ƒç´ çš„æƒé‡\n",
    "y_vec.backward(gradient_weights)\n",
    "\n",
    "print(f\"x_vec = {x_vec.data}\")\n",
    "print(f\"y_vec = x_vecÂ² = {y_vec.data}\")\n",
    "print(f\"æ¢¯åº¦ dy/dx = 2x = {x_vec.grad}\")\n",
    "print(f\"éªŒè¯: [2Ã—1, 2Ã—2, 2Ã—3] = [2, 4, 6] âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 æ¢¯åº¦è®¡ç®—çš„é«˜çº§ç‰¹æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ é«˜çº§æ¢¯åº¦ç‰¹æ€§\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. retain_graphï¼šä¿ç•™è®¡ç®—å›¾\n",
    "print(\"ğŸ”„ retain_graph ä½¿ç”¨:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y ** 2  # z = xâ´\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡åå‘ä¼ æ’­ï¼Œä¿ç•™å›¾\n",
    "z.backward(retain_graph=True)\n",
    "print(f\"ç¬¬ä¸€æ¬¡åå‘ä¼ æ’­ dz/dx = {x.grad}  # 4xÂ³ = 4Ã—8 = 32\")\n",
    "\n",
    "# ç¬¬äºŒæ¬¡åå‘ä¼ æ’­ï¼ˆå¦‚æœä¸ä¿ç•™å›¾ï¼Œè¿™é‡Œä¼šå‡ºé”™ï¼‰\n",
    "x.grad.zero_()  # æ¸…é›¶æ¢¯åº¦\n",
    "z.backward()  # å†æ¬¡è®¡ç®—ï¼ˆå› ä¸ºå›¾è¢«ä¿ç•™äº†ï¼‰\n",
    "print(f\"ç¬¬äºŒæ¬¡åå‘ä¼ æ’­ dz/dx = {x.grad}  # ç»“æœç›¸åŒ\")\n",
    "print()\n",
    "\n",
    "# 2. create_graphï¼šåˆ›å»ºæ¢¯åº¦çš„æ¢¯åº¦ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰\n",
    "print(\"ğŸ“ˆ äºŒé˜¶å¯¼æ•°è®¡ç®—:\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3  # y = xÂ³\n",
    "\n",
    "# è®¡ç®—ä¸€é˜¶å¯¼æ•°\n",
    "grad_first = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"ä¸€é˜¶å¯¼æ•° dy/dx = 3xÂ² = {grad_first}\")\n",
    "\n",
    "# è®¡ç®—äºŒé˜¶å¯¼æ•°\n",
    "grad_second = torch.autograd.grad(grad_first, x)[0]\n",
    "print(f\"äºŒé˜¶å¯¼æ•° dÂ²y/dxÂ² = 6x = {grad_second}\")\n",
    "print(f\"éªŒè¯: 6Ã—{x.item()} = {6*x.item()} âœ“\")\n",
    "print()\n",
    "\n",
    "# 3. torch.autograd.grad vs .backward()\n",
    "print(\"ğŸ”§ torch.autograd.grad vs .backward():\")\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# æ–¹æ³•1ï¼šä½¿ç”¨ .backward()ï¼ˆä¿®æ”¹ x.gradï¼‰\n",
    "y.backward()\n",
    "print(f\"æ–¹æ³•1 (.backward): x.grad = {x.grad}\")\n",
    "\n",
    "# æ–¹æ³•2ï¼šä½¿ç”¨ torch.autograd.gradï¼ˆè¿”å›æ¢¯åº¦å€¼ï¼‰\n",
    "x.grad = None  # æ¸…ç©ºæ¢¯åº¦\n",
    "y = x ** 2  # é‡æ–°è®¡ç®—ï¼ˆéœ€è¦é‡æ–°æ„å»ºå›¾ï¼‰\n",
    "grad_value = torch.autograd.grad(y, x)[0]\n",
    "print(f\"æ–¹æ³•2 (autograd.grad): è¿”å›å€¼ = {grad_value}\")\n",
    "print(f\"æ–¹æ³•2 ä¸ä¿®æ”¹ x.grad: {x.grad}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ä¸¤ç§æ–¹æ³•çš„åŒºåˆ«:\")\n",
    "print(\"- .backward(): å°†æ¢¯åº¦å­˜å‚¨åœ¨ .grad å±æ€§ä¸­\")\n",
    "print(\"- autograd.grad(): è¿”å›æ¢¯åº¦å€¼ï¼Œä¸ä¿®æ”¹ .grad å±æ€§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 4. å®é™…åº”ç”¨ï¼šçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™\n",
    "\n",
    "è®©æˆ‘ä»¬ç”¨è‡ªåŠ¨æ±‚å¯¼æ¥å®ç°ä¸€ä¸ªå®Œæ•´çš„çº¿æ€§å›å½’è®­ç»ƒè¿‡ç¨‹ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ å®æˆ˜ï¼šç”¨è‡ªåŠ¨æ±‚å¯¼å®ç°çº¿æ€§å›å½’\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "# çœŸå®å…³ç³»ï¼šy = 2x + 1 + noise\n",
    "torch.manual_seed(42)\n",
    "n_samples = 100\n",
    "true_w = 2.0  # çœŸå®æ–œç‡\n",
    "true_b = 1.0  # çœŸå®æˆªè·\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®\n",
    "X = torch.randn(n_samples, 1)  # è¾“å…¥ç‰¹å¾\n",
    "noise = torch.randn(n_samples, 1) * 0.3  # å™ªå£°\n",
    "y_true = true_w * X + true_b + noise  # çœŸå®æ ‡ç­¾\n",
    "\n",
    "print(f\"ğŸ“Š æ•°æ®ç”Ÿæˆå®Œæˆ:\")\n",
    "print(f\"æ ·æœ¬æ•°é‡: {n_samples}\")\n",
    "print(f\"çœŸå®å‚æ•°: w = {true_w}, b = {true_b}\")\n",
    "print(f\"è¾“å…¥èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"è¾“å‡ºèŒƒå›´: [{y_true.min():.2f}, {y_true.max():.2f}]\")\n",
    "print()\n",
    "\n",
    "# 2. åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼ˆéœ€è¦æ¢¯åº¦ï¼ï¼‰\n",
    "w = torch.randn(1, 1, requires_grad=True)  # æƒé‡\n",
    "b = torch.randn(1, requires_grad=True)     # åç½®\n",
    "\n",
    "print(f\"ğŸ² åˆå§‹å‚æ•°:\")\n",
    "print(f\"åˆå§‹ w = {w.item():.4f}\")\n",
    "print(f\"åˆå§‹ b = {b.item():.4f}\")\n",
    "print()\n",
    "\n",
    "# 3. å®šä¹‰å‰å‘ä¼ æ’­å’ŒæŸå¤±å‡½æ•°\n",
    "def forward(X, w, b):\n",
    "    \"\"\"çº¿æ€§å›å½’å‰å‘ä¼ æ’­\"\"\"\n",
    "    return X @ w + b  # çŸ©é˜µä¹˜æ³• + å¹¿æ’­\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°\"\"\"\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "# 4. è®­ç»ƒå¾ªç¯\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "losses = []  # è®°å½•æŸå¤±å€¼\n",
    "\n",
    "print(f\"ğŸš€ å¼€å§‹è®­ç»ƒ:\")\n",
    "print(f\"å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(f\"è®­ç»ƒè½®æ•°: {num_epochs}\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    y_pred = forward(X, w, b)\n",
    "    loss = mse_loss(y_pred, y_true)\n",
    "    \n",
    "    # åå‘ä¼ æ’­\n",
    "    loss.backward()  # è®¡ç®—æ¢¯åº¦\n",
    "    \n",
    "    # å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰\n",
    "    with torch.no_grad():  # æ›´æ–°å‚æ•°æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # æ¸…é›¶æ¢¯åº¦ï¼ˆé‡è¦ï¼ï¼‰\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # è®°å½•æŸå¤±\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # æ‰“å°è¿›åº¦\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: Loss = {loss.item():.6f}, w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(f\"âœ… è®­ç»ƒå®Œæˆ!\")\n",
    "print(f\"æœ€ç»ˆå‚æ•°: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print(f\"çœŸå®å‚æ•°: w = {true_w:.4f}, b = {true_b:.4f}\")\n",
    "print(f\"å‚æ•°è¯¯å·®: wè¯¯å·® = {abs(w.item() - true_w):.4f}, bè¯¯å·® = {abs(b.item() - true_b):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# æŸå¤±æ›²çº¿\n",
    "ax1.plot(losses, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# æ•°æ®å’Œæ‹Ÿåˆç»“æœ\n",
    "with torch.no_grad():\n",
    "    # åˆ›å»ºæµ‹è¯•æ•°æ®ç”¨äºç»˜åˆ¶ç›´çº¿\n",
    "    x_test = torch.linspace(X.min(), X.max(), 100).unsqueeze(1)\n",
    "    y_pred_test = forward(x_test, w, b)\n",
    "    y_true_test = true_w * x_test + true_b\n",
    "\n",
    "# æ•£ç‚¹å›¾å’Œæ‹Ÿåˆç›´çº¿\n",
    "ax2.scatter(X.numpy(), y_true.numpy(), alpha=0.6, label='çœŸå®æ•°æ®', s=20)\n",
    "ax2.plot(x_test.numpy(), y_pred_test.numpy(), 'r-', \n",
    "         label=f'å­¦ä¹ åˆ°çš„ç›´çº¿ (w={w.item():.2f}, b={b.item():.2f})', linewidth=2)\n",
    "ax2.plot(x_test.numpy(), y_true_test.numpy(), 'g--', \n",
    "         label=f'çœŸå®ç›´çº¿ (w={true_w:.2f}, b={true_b:.2f})', linewidth=2)\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('çº¿æ€§å›å½’æ‹Ÿåˆç»“æœ')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ‰ å¤ªæ£’äº†ï¼è‡ªåŠ¨æ±‚å¯¼å¸®æˆ‘ä»¬å®Œç¾åœ°å­¦ä¹ åˆ°äº†æ•°æ®çš„æ½œåœ¨è§„å¾‹ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš¨ 5. å¸¸è§é™·é˜±å’Œæ³¨æ„äº‹é¡¹\n",
    "\n",
    "åœ¨ä½¿ç”¨è‡ªåŠ¨æ±‚å¯¼æ—¶ï¼Œæœ‰ä¸€äº›å¸¸è§çš„é™·é˜±éœ€è¦æ³¨æ„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš¨ å¸¸è§é™·é˜±å’Œè§£å†³æ–¹æ¡ˆ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# é™·é˜±1ï¼šå¿˜è®°æ¸…é›¶æ¢¯åº¦\n",
    "print(\"âš ï¸ é™·é˜±1: å¿˜è®°æ¸…é›¶æ¢¯åº¦\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    print(f\"ç¬¬{i+1}æ¬¡: x.grad = {x.grad}  # æ¢¯åº¦ç´¯ç§¯äº†ï¼\")\n",
    "    # æ­£ç¡®åšæ³•ï¼šæ¯æ¬¡éƒ½è¦æ¸…é›¶\n",
    "    # x.grad.zero_()  # å–æ¶ˆæ³¨é‡Šä»¥ä¿®å¤\n",
    "\n",
    "print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ: æ¯æ¬¡backward()å‰è°ƒç”¨ x.grad.zero_()\\n\")\n",
    "\n",
    "# é™·é˜±2ï¼šè¯•å›¾å¯¹éæ ‡é‡è¿›è¡Œbackward\n",
    "print(\"âš ï¸ é™·é˜±2: å¯¹éæ ‡é‡è¿›è¡Œbackward\")\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = x ** 2  # ç»“æœæ˜¯å‘é‡\n",
    "try:\n",
    "    y.backward()  # è¿™ä¼šå‡ºé”™ï¼\n",
    "except RuntimeError as e:\n",
    "    print(f\"é”™è¯¯: {str(e)[:50]}...\")\n",
    "    print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ1: å…ˆæ±‚å’Œå†backward\")\n",
    "    y.sum().backward()\n",
    "    print(f\"æ±‚å’ŒåbackwardæˆåŠŸ: x.grad = {x.grad}\")\n",
    "    \n",
    "# æˆ–è€…æä¾›gradientå‚æ•°\n",
    "x.grad.zero_()\n",
    "y = x ** 2\n",
    "gradient_weights = torch.tensor([1.0, 1.0])\n",
    "y.backward(gradient_weights)\n",
    "print(f\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ2: æä¾›gradientå‚æ•°: x.grad = {x.grad}\\n\")\n",
    "\n",
    "# é™·é˜±3ï¼šin-placeæ“ä½œç ´åè®¡ç®—å›¾\n",
    "print(\"âš ï¸ é™·é˜±3: in-placeæ“ä½œç ´åè®¡ç®—å›¾\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "try:\n",
    "    y += 1  # in-placeæ“ä½œï¼Œç­‰ä»·äº y.add_(1)\n",
    "    y.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f\"é”™è¯¯: in-placeæ“ä½œå¯èƒ½ç ´åè®¡ç®—å›¾\")\n",
    "    print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨éin-placeæ“ä½œ\")\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x ** 2\n",
    "    z = y + 1  # éin-placeæ“ä½œ\n",
    "    z.backward()\n",
    "    print(f\"ä¿®å¤å: x.grad = {x.grad}\\n\")\n",
    "\n",
    "# é™·é˜±4ï¼šåœ¨no_gradä¸Šä¸‹æ–‡ä¸­æ„å¤–å…³é—­æ¢¯åº¦\n",
    "print(\"âš ï¸ é™·é˜±4: æ„å¤–å…³é—­æ¢¯åº¦è®¡ç®—\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x ** 2  # åœ¨no_gradä¸­è®¡ç®—\n",
    "print(f\"y.requires_grad = {y.requires_grad}  # Falseï¼æ¢¯åº¦è¢«å…³é—­äº†\")\n",
    "print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ: ç¡®ä¿éœ€è¦æ¢¯åº¦çš„è®¡ç®—åœ¨no_gradä¹‹å¤–\\n\")\n",
    "\n",
    "# é™·é˜±5ï¼šé‡å¤ä½¿ç”¨ä¸­é—´å˜é‡\n",
    "print(\"âš ï¸ é™·é˜±5: é‡å¤backwardåŒä¸€ä¸ªå›¾\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()  # ç¬¬ä¸€æ¬¡backward\n",
    "try:\n",
    "    y.backward()  # ç¬¬äºŒæ¬¡backwardï¼Œå›¾å·²ç»è¢«é‡Šæ”¾ï¼\n",
    "except RuntimeError as e:\n",
    "    print(f\"é”™è¯¯: è®¡ç®—å›¾å·²è¢«é‡Šæ”¾\")\n",
    "    print(\"ğŸ’¡ è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ retain_graph=True æˆ–é‡æ–°è®¡ç®—\")\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    y = x ** 2\n",
    "    y.backward(retain_graph=True)  # ä¿ç•™å›¾\n",
    "    x.grad.zero_()  # æ¸…é›¶æ¢¯åº¦\n",
    "    y.backward()  # ç°åœ¨å¯ä»¥å†æ¬¡backward\n",
    "    print(f\"ä¿®å¤å: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 6. è‡ªåŠ¨æ±‚å¯¼çš„é«˜çº§åº”ç”¨\n",
    "\n",
    "è®©æˆ‘ä»¬æ¢ç´¢ä¸€äº›é«˜çº§åº”ç”¨åœºæ™¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ é«˜çº§åº”ç”¨åœºæ™¯\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. è®¡ç®—é›…å¯æ¯”çŸ©é˜µ (Jacobian)\n",
    "print(\"ğŸ“Š è®¡ç®—é›…å¯æ¯”çŸ©é˜µ:\")\n",
    "\n",
    "def compute_jacobian(func, inputs):\n",
    "    \"\"\"è®¡ç®—å‡½æ•°çš„é›…å¯æ¯”çŸ©é˜µ\"\"\"\n",
    "    outputs = func(inputs)\n",
    "    jacobian = torch.zeros(outputs.size(0), inputs.size(0))\n",
    "    \n",
    "    for i in range(outputs.size(0)):\n",
    "        # ä¸ºæ¯ä¸ªè¾“å‡ºåˆ†é‡è®¡ç®—æ¢¯åº¦\n",
    "        grad_outputs = torch.zeros_like(outputs)\n",
    "        grad_outputs[i] = 1.0\n",
    "        \n",
    "        inputs.grad = None  # æ¸…é›¶æ¢¯åº¦\n",
    "        grad_inputs = torch.autograd.grad(outputs, inputs, \n",
    "                                        grad_outputs=grad_outputs,\n",
    "                                        retain_graph=True)[0]\n",
    "        jacobian[i] = grad_inputs\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# ç¤ºä¾‹ï¼šf(x) = [x1Â², x1*x2, x2Â²]\n",
    "def vector_function(x):\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return torch.stack([x1**2, x1*x2, x2**2])\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "jacobian = compute_jacobian(vector_function, x)\n",
    "\n",
    "print(f\"è¾“å…¥: x = {x.data}\")\n",
    "print(f\"å‡½æ•°: f(x) = [x1Â², x1*x2, x2Â²]\")\n",
    "print(f\"é›…å¯æ¯”çŸ©é˜µ:\\n{jacobian}\")\n",
    "print(f\"ç†è®ºå€¼:\")\n",
    "print(f\"df1/dx1 = 2*x1 = {2*x[0]}, df1/dx2 = 0\")\n",
    "print(f\"df2/dx1 = x2 = {x[1]}, df2/dx2 = x1 = {x[0]}\")\n",
    "print(f\"df3/dx1 = 0, df3/dx2 = 2*x2 = {2*x[1]}\\n\")\n",
    "\n",
    "# 2. æ¢¯åº¦æ£€æŸ¥ (Gradient Checking)\n",
    "print(\"ğŸ” æ¢¯åº¦æ£€æŸ¥:\")\n",
    "\n",
    "def numerical_gradient(func, x, eps=1e-7):\n",
    "    \"\"\"æ•°å€¼æ–¹æ³•è®¡ç®—æ¢¯åº¦\"\"\"\n",
    "    grad = torch.zeros_like(x)\n",
    "    for i in range(x.size(0)):\n",
    "        x_plus = x.clone()\n",
    "        x_minus = x.clone()\n",
    "        x_plus[i] += eps\n",
    "        x_minus[i] -= eps\n",
    "        \n",
    "        grad[i] = (func(x_plus) - func(x_minus)) / (2 * eps)\n",
    "    return grad\n",
    "\n",
    "def test_function(x):\n",
    "    return (x ** 3).sum()  # f(x) = x1Â³ + x2Â³\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# è‡ªåŠ¨æ±‚å¯¼\n",
    "y = test_function(x)\n",
    "y.backward()\n",
    "auto_grad = x.grad.clone()\n",
    "\n",
    "# æ•°å€¼æ±‚å¯¼\n",
    "x_no_grad = torch.tensor([2.0, 3.0])  # ä¸éœ€è¦æ¢¯åº¦çš„ç‰ˆæœ¬\n",
    "numerical_grad = numerical_gradient(test_function, x_no_grad)\n",
    "\n",
    "print(f\"è‡ªåŠ¨æ±‚å¯¼æ¢¯åº¦: {auto_grad}\")\n",
    "print(f\"æ•°å€¼æ±‚å¯¼æ¢¯åº¦: {numerical_grad}\")\n",
    "print(f\"å·®å¼‚: {torch.abs(auto_grad - numerical_grad)}\")\n",
    "print(f\"ç›¸å¯¹è¯¯å·®: {torch.abs(auto_grad - numerical_grad) / torch.abs(auto_grad)}\")\n",
    "print(\"âœ… æ¢¯åº¦æ£€æŸ¥é€šè¿‡ï¼(å·®å¼‚æå°)\\n\")\n",
    "\n",
    "# 3. æ¡ä»¶æ¢¯åº¦è®¡ç®—\n",
    "print(\"ğŸ­ æ¡ä»¶æ¢¯åº¦è®¡ç®—:\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "condition = x > 1.0\n",
    "\n",
    "if condition:\n",
    "    y = x ** 2\n",
    "else:\n",
    "    y = x ** 3\n",
    "\n",
    "y.backward()\n",
    "print(f\"å½“ x = {x.item()} > 1 æ—¶:\")\n",
    "print(f\"ä½¿ç”¨å‡½æ•° y = xÂ²\")\n",
    "print(f\"æ¢¯åº¦ dy/dx = 2x = {x.grad}\")\n",
    "\n",
    "# æµ‹è¯•å¦ä¸€ä¸ªæ¡ä»¶\n",
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "if x > 1.0:\n",
    "    y = x ** 2\n",
    "else:\n",
    "    y = x ** 3\n",
    "\n",
    "y.backward()\n",
    "print(f\"\\nå½“ x = {x.item()} â‰¤ 1 æ—¶:\")\n",
    "print(f\"ä½¿ç”¨å‡½æ•° y = xÂ³\")\n",
    "print(f\"æ¢¯åº¦ dy/dx = 3xÂ² = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 7. å®æˆ˜ï¼šå¤šå±‚æ„ŸçŸ¥æœºçš„åå‘ä¼ æ’­\n",
    "\n",
    "è®©æˆ‘ä»¬æ‰‹åŠ¨å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Œæ·±å…¥ç†è§£åå‘ä¼ æ’­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§  å®æˆ˜ï¼šæ‰‹åŠ¨å®ç°ç¥ç»ç½‘ç»œåå‘ä¼ æ’­\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"ç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ\n",
    "        \n",
    "        ç½‘ç»œç»“æ„: input â†’ hidden â†’ output\n",
    "        æ¿€æ´»å‡½æ•°: ReLU (éšè—å±‚), æ— æ¿€æ´» (è¾“å‡ºå±‚)\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–æƒé‡ï¼ˆéœ€è¦æ¢¯åº¦ï¼ï¼‰\n",
    "        self.W1 = torch.randn(input_size, hidden_size, requires_grad=True) * 0.5\n",
    "        self.b1 = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        self.W2 = torch.randn(hidden_size, output_size, requires_grad=True) * 0.5\n",
    "        self.b2 = torch.zeros(output_size, requires_grad=True)\n",
    "        \n",
    "        print(f\"ğŸ—ï¸ ç½‘ç»œç»“æ„: {input_size} â†’ {hidden_size} â†’ {output_size}\")\n",
    "        print(f\"å‚æ•°æ•°é‡: {self.count_parameters()}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
    "        # ç¬¬ä¸€å±‚: input â†’ hidden\n",
    "        z1 = x @ self.W1 + self.b1  # çº¿æ€§å˜æ¢\n",
    "        a1 = torch.relu(z1)         # ReLUæ¿€æ´»\n",
    "        \n",
    "        # ç¬¬äºŒå±‚: hidden â†’ output\n",
    "        z2 = a1 @ self.W2 + self.b2  # çº¿æ€§å˜æ¢\n",
    "        \n",
    "        return z2\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"ç»Ÿè®¡å‚æ•°æ•°é‡\"\"\"\n",
    "        total = 0\n",
    "        for param in [self.W1, self.b1, self.W2, self.b2]:\n",
    "            total += param.numel()\n",
    "        return total\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"æ¸…é›¶æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\"\"\"\n",
    "        for param in [self.W1, self.b1, self.W2, self.b2]:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"è·å–æ‰€æœ‰å‚æ•°\"\"\"\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "# åˆ›å»ºç½‘ç»œå’Œæ•°æ®\n",
    "torch.manual_seed(42)\n",
    "net = SimpleNeuralNetwork(input_size=3, hidden_size=5, output_size=2)\n",
    "\n",
    "# ç”Ÿæˆä¸€æ‰¹æ ·æœ¬æ•°æ®\n",
    "batch_size = 4\n",
    "X = torch.randn(batch_size, 3)  # 4ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª3ç»´ç‰¹å¾\n",
    "y_true = torch.randn(batch_size, 2)  # 4ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª2ç»´è¾“å‡º\n",
    "\n",
    "print(f\"\\nğŸ“Š æ•°æ®ä¿¡æ¯:\")\n",
    "print(f\"è¾“å…¥å½¢çŠ¶: {X.shape}\")\n",
    "print(f\"ç›®æ ‡å½¢çŠ¶: {y_true.shape}\")\n",
    "print()\n",
    "\n",
    "# è®­ç»ƒä¸€ä¸ªstepï¼Œè¯¦ç»†è§‚å¯Ÿæ¢¯åº¦\n",
    "print(f\"ğŸ”„ è¯¦ç»†çš„ä¸€æ­¥è®­ç»ƒè¿‡ç¨‹:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. å‰å‘ä¼ æ’­\n",
    "print(\"1ï¸âƒ£ å‰å‘ä¼ æ’­:\")\n",
    "y_pred = net.forward(X)\n",
    "print(f\"é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {y_pred.shape}\")\n",
    "print(f\"é¢„æµ‹å€¼èŒƒå›´: [{y_pred.min():.3f}, {y_pred.max():.3f}]\")\n",
    "\n",
    "# 2. è®¡ç®—æŸå¤±\n",
    "print(\"\\n2ï¸âƒ£ è®¡ç®—æŸå¤±:\")\n",
    "loss = torch.nn.functional.mse_loss(y_pred, y_true)\n",
    "print(f\"MSEæŸå¤±: {loss.item():.6f}\")\n",
    "\n",
    "# 3. åå‘ä¼ æ’­\n",
    "print(\"\\n3ï¸âƒ£ åå‘ä¼ æ’­:\")\n",
    "print(\"è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦...\")\n",
    "loss.backward()\n",
    "\n",
    "# æ£€æŸ¥æ¢¯åº¦\n",
    "print(\"\\nğŸ“Š æ¢¯åº¦ç»Ÿè®¡:\")\n",
    "param_names = ['W1', 'b1', 'W2', 'b2']\n",
    "for name, param in zip(param_names, net.get_parameters()):\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.norm().item()\n",
    "        grad_mean = param.grad.mean().item()\n",
    "        print(f\"{name}: æ¢¯åº¦èŒƒæ•°={grad_norm:.6f}, æ¢¯åº¦å‡å€¼={grad_mean:.6f}\")\n",
    "    else:\n",
    "        print(f\"{name}: æ¢¯åº¦ä¸ºNone\")\n",
    "\n",
    "# 4. å‚æ•°æ›´æ–°\n",
    "print(\"\\n4ï¸âƒ£ å‚æ•°æ›´æ–°:\")\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():\n",
    "    for param in net.get_parameters():\n",
    "        param -= learning_rate * param.grad\n",
    "print(f\"å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(\"å‚æ•°æ›´æ–°å®Œæˆ\")\n",
    "\n",
    "# 5. æ¸…é›¶æ¢¯åº¦\n",
    "print(\"\\n5ï¸âƒ£ æ¸…é›¶æ¢¯åº¦:\")\n",
    "net.zero_grad()\n",
    "print(\"æ¢¯åº¦æ¸…é›¶å®Œæˆ\")\n",
    "\n",
    "print(\"\\nğŸ‰ ä¸€æ­¥å®Œæ•´çš„è®­ç»ƒå¾ªç¯å®Œæˆï¼\")\n",
    "print(\"è¿™å°±æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ ¸å¿ƒè¿‡ç¨‹ï¼šå‰å‘â†’æŸå¤±â†’åå‘â†’æ›´æ–°â†’æ¸…é›¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 å¯è§†åŒ–æ¢¯åº¦æµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_flow(net, X, y_true, num_steps=10):\n",
    "    \"\"\"å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦å˜åŒ–\"\"\"\n",
    "    losses = []\n",
    "    w1_grads = []\n",
    "    w2_grads = []\n",
    "    \n",
    "    print(\"ğŸ“ˆ æ¢¯åº¦æµå¯è§†åŒ– (å‰10æ­¥è®­ç»ƒ)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        y_pred = net.forward(X)\n",
    "        loss = torch.nn.functional.mse_loss(y_pred, y_true)\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # è®°å½•æ•°æ®\n",
    "        losses.append(loss.item())\n",
    "        w1_grads.append(net.W1.grad.norm().item())\n",
    "        w2_grads.append(net.W2.grad.norm().item())\n",
    "        \n",
    "        # å‚æ•°æ›´æ–°\n",
    "        with torch.no_grad():\n",
    "            for param in net.get_parameters():\n",
    "                param -= 0.01 * param.grad\n",
    "        \n",
    "        if step % 3 == 0:\n",
    "            print(f\"Step {step:2d}: Loss={loss.item():.4f}, \"\n",
    "                  f\"W1_grad_norm={w1_grads[-1]:.4f}, \"\n",
    "                  f\"W2_grad_norm={w2_grads[-1]:.4f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶å›¾åƒ\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # æŸå¤±æ›²çº¿\n",
    "    ax1.plot(losses, 'b-', marker='o', markersize=4)\n",
    "    ax1.set_xlabel('Training Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('è®­ç»ƒæŸå¤±å˜åŒ–')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ¢¯åº¦èŒƒæ•°\n",
    "    ax2.plot(w1_grads, 'r-', marker='s', markersize=4, label='W1 æ¢¯åº¦èŒƒæ•°')\n",
    "    ax2.plot(w2_grads, 'g-', marker='^', markersize=4, label='W2 æ¢¯åº¦èŒƒæ•°')\n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Gradient Norm')\n",
    "    ax2.set_title('æ¢¯åº¦èŒƒæ•°å˜åŒ–')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return losses, w1_grads, w2_grads\n",
    "\n",
    "# è¿è¡Œå¯è§†åŒ–\n",
    "losses, w1_grads, w2_grads = visualize_gradient_flow(net, X, y_true)\n",
    "\n",
    "print(f\"\\nğŸ“Š è§‚å¯Ÿç»“æœ:\")\n",
    "print(f\"â€¢ æŸå¤±ä» {losses[0]:.4f} é™è‡³ {losses[-1]:.4f}\")\n",
    "print(f\"â€¢ W1å±‚æ¢¯åº¦èŒƒæ•°: {w1_grads[0]:.4f} â†’ {w1_grads[-1]:.4f}\")\n",
    "print(f\"â€¢ W2å±‚æ¢¯åº¦èŒƒæ•°: {w2_grads[0]:.4f} â†’ {w2_grads[-1]:.4f}\")\n",
    "print(f\"âœ… æ¢¯åº¦æ­£å¸¸æµåŠ¨ï¼Œç½‘ç»œåœ¨å­¦ä¹ ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 8. è‡ªåŠ¨æ±‚å¯¼åŸç†æ·±åº¦è§£æ\n",
    "\n",
    "è®©æˆ‘ä»¬æ·±å…¥ç†è§£è‡ªåŠ¨æ±‚å¯¼çš„åº•å±‚åŸç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¬ è‡ªåŠ¨æ±‚å¯¼åŸç†æ·±åº¦è§£æ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. é“¾å¼æ³•åˆ™çš„é€æ­¥å±•ç¤º\n",
    "print(\"â›“ï¸ é“¾å¼æ³•åˆ™è¯¦ç»†å±•ç¤º:\")\n",
    "print(\"å‡½æ•°: f(x) = sin(xÂ²)\")\n",
    "print(\"åˆ†è§£: u = xÂ², v = sin(u), f = v\")\n",
    "print(\"é“¾å¼æ³•åˆ™: df/dx = (df/dv) Ã— (dv/du) Ã— (du/dx)\")\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# é€æ­¥è®¡ç®—ï¼Œä¿å­˜ä¸­é—´ç»“æœ\n",
    "u = x ** 2\n",
    "print(f\"\\nä¸­é—´æ­¥éª¤:\")\n",
    "print(f\"x = {x.item():.4f}\")\n",
    "print(f\"u = xÂ² = {u.item():.4f}\")\n",
    "\n",
    "v = torch.sin(u)\n",
    "print(f\"v = sin(u) = {v.item():.4f}\")\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "v.backward()\n",
    "print(f\"\\næ¢¯åº¦è®¡ç®—:\")\n",
    "print(f\"df/dx = {x.grad:.6f}\")\n",
    "\n",
    "# æ‰‹åŠ¨éªŒè¯\n",
    "manual_grad = torch.cos(u) * 2 * x\n",
    "print(f\"æ‰‹åŠ¨è®¡ç®—: cos({u.item():.4f}) Ã— 2 Ã— {x.item()} = {manual_grad.item():.6f}\")\n",
    "print(f\"è¯¯å·®: {abs(x.grad.item() - manual_grad.item()):.10f}\\n\")\n",
    "\n",
    "# 2. ä¸åŒè¿ç®—çš„å±€éƒ¨æ¢¯åº¦\n",
    "print(\"ğŸ§® å¸¸è§è¿ç®—çš„å±€éƒ¨æ¢¯åº¦:\")\n",
    "operations = [\n",
    "    (\"åŠ æ³•\", \"z = x + y\", \"dz/dx = 1, dz/dy = 1\"),\n",
    "    (\"ä¹˜æ³•\", \"z = x * y\", \"dz/dx = y, dz/dy = x\"),\n",
    "    (\"æŒ‡æ•°\", \"z = x^n\", \"dz/dx = n * x^(n-1)\"),\n",
    "    (\"å¯¹æ•°\", \"z = log(x)\", \"dz/dx = 1/x\"),\n",
    "    (\"ReLU\", \"z = max(0,x)\", \"dz/dx = 1 if x>0 else 0\"),\n",
    "    (\"Sigmoid\", \"z = 1/(1+e^(-x))\", \"dz/dx = z*(1-z)\")\n",
    "]\n",
    "\n",
    "for name, func, grad in operations:\n",
    "    print(f\"{name:8s}: {func:15s} â†’ {grad}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ è‡ªåŠ¨æ±‚å¯¼å°±æ˜¯è‡ªåŠ¨åº”ç”¨è¿™äº›è§„åˆ™ï¼\")\n",
    "\n",
    "# 3. è®¡ç®—å›¾çš„å†…å­˜ç®¡ç†\n",
    "print(\"\\nğŸ’¾ è®¡ç®—å›¾çš„å†…å­˜ç®¡ç†:\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y * 3\n",
    "\n",
    "print(f\"åˆ›å»ºè®¡ç®—å›¾: x â†’ y â†’ z\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")\n",
    "print(f\"x.grad_fn: {x.grad_fn}\")\n",
    "\n",
    "# æ£€æŸ¥å¼•ç”¨è®¡æ•°\n",
    "import sys\n",
    "print(f\"\\nPythonå¼•ç”¨è®¡æ•°:\")\n",
    "print(f\"xå¼•ç”¨æ•°: {sys.getrefcount(x)}\")\n",
    "print(f\"yå¼•ç”¨æ•°: {sys.getrefcount(y)}\")\n",
    "print(f\"zå¼•ç”¨æ•°: {sys.getrefcount(z)}\")\n",
    "\n",
    "# backwardåå›¾è¢«é‡Šæ”¾\n",
    "z.backward()\n",
    "print(f\"\\nbackwardå:\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}  # å›¾è¢«é‡Šæ”¾\")\n",
    "\n",
    "# 4. åŠ¨æ€è®¡ç®—å›¾ vs é™æ€è®¡ç®—å›¾\n",
    "print(\"\\nğŸ”„ åŠ¨æ€è®¡ç®—å›¾çš„ä¼˜åŠ¿:\")\n",
    "\n",
    "for i in range(3):\n",
    "    x = torch.tensor(float(i+1), requires_grad=True)\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        y = x ** 2  # å¶æ•°æ¬¡è¿­ä»£\n",
    "    else:\n",
    "        y = x ** 3  # å¥‡æ•°æ¬¡è¿­ä»£\n",
    "    \n",
    "    y.backward()\n",
    "    print(f\"è¿­ä»£{i}: x={x.item()}, å‡½æ•°=x^{2 if i%2==0 else 3}, æ¢¯åº¦={x.grad.item()}\")\n",
    "\n",
    "print(\"ğŸ’¡ æ¯æ¬¡å‰å‘ä¼ æ’­éƒ½å¯ä»¥ä½¿ç”¨ä¸åŒçš„è®¡ç®—å›¾ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ æ€»ç»“ä¸å…³é”®è¦ç‚¹\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒæ¦‚å¿µå›é¡¾\n",
    "\n",
    "1. **è‡ªåŠ¨æ±‚å¯¼æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºçŸ³** - è®©æˆ‘ä»¬æ— éœ€æ‰‹åŠ¨æ¨å¯¼å¤æ‚çš„æ¢¯åº¦å…¬å¼\n",
    "2. **è®¡ç®—å›¾è®°å½•è¿ç®—å†å²** - å‰å‘ä¼ æ’­æ„å»ºå›¾ï¼Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦\n",
    "3. **é“¾å¼æ³•åˆ™è‡ªåŠ¨åº”ç”¨** - PyTorchè‡ªåŠ¨å¤„ç†å¤åˆå‡½æ•°çš„æ¢¯åº¦è®¡ç®—\n",
    "4. **åŠ¨æ€å›¾çš„çµæ´»æ€§** - æ¯æ¬¡å‰å‘ä¼ æ’­éƒ½å¯ä»¥ä½¿ç”¨ä¸åŒçš„è®¡ç®—é€»è¾‘\n",
    "\n",
    "### ğŸ’¡ å®ç”¨æŠ€å·§æ€»ç»“\n",
    "\n",
    "- âœ… **å§‹ç»ˆè®°ä½æ¸…é›¶æ¢¯åº¦**: `tensor.grad.zero_()` æˆ– `tensor.grad = None`\n",
    "- âœ… **åˆç†ä½¿ç”¨ `requires_grad`**: åªä¸ºéœ€è¦æ¢¯åº¦çš„å¼ é‡è®¾ç½®\n",
    "- âœ… **é¿å… in-place æ“ä½œ**: ä½¿ç”¨ `y = x + 1` è€Œä¸æ˜¯ `x += 1`\n",
    "- âœ… **ç†è§£ `no_grad` ä¸Šä¸‹æ–‡**: æ¨ç†æ—¶å…³é—­æ¢¯åº¦è®¡ç®—èŠ‚çœå†…å­˜\n",
    "- âœ… **ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥éªŒè¯**: æ•°å€¼æ¢¯åº¦ vs è‡ªåŠ¨æ¢¯åº¦\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **ä¼˜åŒ–å™¨ (Optimizers)** - SGD, Adam, RMSpropç­‰\n",
    "2. **æŸå¤±å‡½æ•° (Loss Functions)** - å„ç§æŸå¤±å‡½æ•°çš„è®¾è®¡åŸç†\n",
    "3. **ç¥ç»ç½‘ç»œæ¨¡å— (nn.Module)** - æ„å»ºå¤æ‚æ¨¡å‹çš„åŸºç¡€\n",
    "4. **é«˜çº§è‡ªåŠ¨æ±‚å¯¼** - é«˜é˜¶å¯¼æ•°ã€é›…å¯æ¯”çŸ©é˜µç­‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ† æ­å–œå®Œæˆè‡ªåŠ¨æ±‚å¯¼æœºåˆ¶å­¦ä¹ ï¼\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# æœ€ç»ˆæµ‹è¯•ï¼šç»¼åˆåº”ç”¨\n",
    "print(\"ğŸ¯ ç»¼åˆæµ‹è¯•: å®ç°ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜\")\n",
    "print(\"é—®é¢˜: æ‰¾åˆ°å‡½æ•° f(x,y) = (x-3)Â² + (y-2)Â² çš„æœ€å°å€¼\")\n",
    "print(\"ç†è®ºç­”æ¡ˆ: x=3, y=2æ—¶è¾¾åˆ°æœ€å°å€¼0\")\n",
    "\n",
    "# ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ±‚è§£\n",
    "x = torch.tensor(0.0, requires_grad=True)  # èµ·å§‹ç‚¹ (0, 0)\n",
    "y = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "for step in range(50):\n",
    "    # ç›®æ ‡å‡½æ•°\n",
    "    f = (x - 3)**2 + (y - 2)**2\n",
    "    \n",
    "    # åå‘ä¼ æ’­\n",
    "    f.backward()\n",
    "    \n",
    "    # æ¢¯åº¦ä¸‹é™æ›´æ–°\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "        y -= learning_rate * y.grad\n",
    "    \n",
    "    # æ¸…é›¶æ¢¯åº¦\n",
    "    x.grad.zero_()\n",
    "    y.grad.zero_()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:2d}: f={f.item():.6f}, x={x.item():.4f}, y={y.item():.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ æœ€ç»ˆç»“æœ:\")\n",
    "print(f\"æ‰¾åˆ°çš„æœ€ä¼˜è§£: x = {x.item():.4f}, y = {y.item():.4f}\")\n",
    "print(f\"ç†è®ºæœ€ä¼˜è§£:   x = 3.0000, y = 2.0000\")\n",
    "print(f\"æœ€å°å‡½æ•°å€¼:   f = {f.item():.8f}\")\n",
    "\n",
    "error = ((x.item()-3)**2 + (y.item()-2)**2)**0.5\n",
    "print(f\"è¯¯å·®è·ç¦»:     {error:.6f}\")\n",
    "\n",
    "if error < 0.01:\n",
    "    print(\"âœ… ä¼˜åŒ–æˆåŠŸï¼è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…\")\n",
    "else:\n",
    "    print(\"âŒ éœ€è¦æ›´å¤šè®­ç»ƒæ­¥æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡\")\n",
    "\n",
    "print(\"\\nğŸ’ª ä½ å·²ç»æŒæ¡äº†è‡ªåŠ¨æ±‚å¯¼çš„æ ¸å¿ƒåŸç†å’Œåº”ç”¨ï¼\")\n",
    "print(\"ç°åœ¨å¯ä»¥å¼€å§‹å­¦ä¹ æ›´é«˜çº§çš„æ·±åº¦å­¦ä¹ æ¦‚å¿µäº†ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}