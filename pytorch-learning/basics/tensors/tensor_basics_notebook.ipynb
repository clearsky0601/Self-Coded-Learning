{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch 张量基础教程\n",
    "\n",
    "## 📚 本节学习目标\n",
    "- 理解张量(Tensor)的基本概念\n",
    "- 掌握张量的创建方法\n",
    "- 学会张量的基本操作和属性\n",
    "- 熟悉张量的形状变换\n",
    "- 掌握张量的数学运算\n",
    "- 了解张量与NumPy的关系\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 PyTorch 版本: 2.7.1+cu126\n",
      "🚀 使用 CUDA: NVIDIA GeForce GTX 1050 Ti\n",
      "📊 CUDA 内存: 4.2 GB\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到路径,定位项目中的文件或目录，方便跨模块引用。\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from utils.common import get_device, print_tensor_info\n",
    "\n",
    "print(f\"🔥 PyTorch 版本: {torch.__version__}\")\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 1. 什么是张量？\n",
    "\n",
    "**张量(Tensor)** 是PyTorch中最基本的数据结构，可以理解为：\n",
    "\n",
    "- **0维张量**: 标量 (scalar) - 一个数字\n",
    "- **1维张量**: 向量 (vector) - 一行数字\n",
    "- **2维张量**: 矩阵 (matrix) - 表格形式的数据\n",
    "- **3维张量**: 立方体 - 比如RGB图像 (高×宽×通道)\n",
    "- **4维张量**: 一批图像 (批次×通道×高×宽)\n",
    "- **更高维**: 复杂的多维数据\n",
    "\n",
    "### 🎭 生活中的类比\n",
    "- **标量**: 温度 (23.5°C)\n",
    "- **向量**: 一天的温度记录 [20, 22, 25, 28, 26]\n",
    "- **矩阵**: 一周的温度记录表\n",
    "- **3维张量**: 一个月每天每小时的温度记录\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ 2. 张量的创建方法\n",
    "\n",
    "让我们学习各种创建张量的方法：\n",
    "\n",
    "### 2.1 从数据直接创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 从列表创建张量：\n",
      "📊 标量 信息:\n",
      "  形状: torch.Size([])\n",
      "  数据类型: torch.int64\n",
      "  设备: cpu\n",
      "  内存占用: 0.00 MB\n",
      "  值: 42\n",
      "\n",
      "📊 向量 信息:\n",
      "  形状: torch.Size([5])\n",
      "  数据类型: torch.int64\n",
      "  设备: cpu\n",
      "  内存占用: 0.00 MB\n",
      "  值: tensor([1, 2, 3, 4, 5])\n",
      "\n",
      "📊 矩阵 信息:\n",
      "  形状: torch.Size([2, 3])\n",
      "  数据类型: torch.int64\n",
      "  设备: cpu\n",
      "  内存占用: 0.00 MB\n",
      "  值: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "📊 3维张量 信息:\n",
      "  形状: torch.Size([2, 2, 2])\n",
      "  数据类型: torch.int64\n",
      "  设备: cpu\n",
      "  内存占用: 0.00 MB\n",
      "  值: tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# 📊 从Python列表创建张量\n",
    "print(\"🔢 从列表创建张量：\")\n",
    "\n",
    "# 0维张量 (标量)\n",
    "scalar = torch.tensor(42)\n",
    "print_tensor_info(scalar, \"标量\")\n",
    "print()\n",
    "\n",
    "# 1维张量 (向量)\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "print_tensor_info(vector, \"向量\")\n",
    "print()\n",
    "\n",
    "# 2维张量 (矩阵)\n",
    "matrix = torch.tensor([[1, 2, 3], \n",
    "                      [4, 5, 6]])\n",
    "print_tensor_info(matrix, \"矩阵\")\n",
    "print()\n",
    "\n",
    "# 3维张量\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], \n",
    "                         [[5, 6], [7, 8]]])\n",
    "print_tensor_info(tensor_3d, \"3维张量\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 特殊值张量的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 创建特殊值张量：\n",
      "全零张量 (3x4):\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "全一张量 (2x3):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "单位矩阵 (3x3):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "填充7.5的张量 (2x3):\n",
      "tensor([[7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(\"🎨 创建特殊值张量：\")\n",
    "\n",
    "# 全零张量\n",
    "zeros = torch.zeros(3, 4)\n",
    "print(f\"全零张量 (3x4):\\n{zeros}\\n\")\n",
    "\n",
    "# 全一张量\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"全一张量 (2x3):\\n{ones}\\n\")\n",
    "\n",
    "# 对角线为1的单位矩阵\n",
    "eye = torch.eye(3)\n",
    "print(f\"单位矩阵 (3x3):\\n{eye}\\n\")\n",
    "\n",
    "# 填充特定值\n",
    "filled = torch.full((2, 3), 7.5)\n",
    "print(f\"填充7.5的张量 (2x3):\\n{filled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 随机张量的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 创建随机张量：\n",
      "均匀分布随机张量 [0,1):\n",
      "tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "\n",
      "正态分布随机张量 N(0,1):\n",
      "tensor([[ 1.1561,  0.3965, -2.4661],\n",
      "        [ 0.3623,  0.3765, -0.1808]])\n",
      "\n",
      "随机整数张量 [0,10):\n",
      "tensor([[7, 6, 9],\n",
      "        [6, 3, 1]])\n",
      "\n",
      "与matrix相同形状的随机张量:\n",
      "tensor([[ 1.1103, -1.6898, -0.9890],\n",
      "        [ 0.9580,  1.3221,  0.8172]])\n"
     ]
    }
   ],
   "source": [
    "print(\"🎲 创建随机张量：\")\n",
    "\n",
    "# 设置随机种子以便结果可重现\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 0-1之间的均匀分布随机数\n",
    "random_uniform = torch.rand(2, 3)\n",
    "print(f\"均匀分布随机张量 [0,1):\\n{random_uniform}\\n\")\n",
    "\n",
    "# 标准正态分布随机数 (均值0，标准差1)\n",
    "random_normal = torch.randn(2, 3)\n",
    "print(f\"正态分布随机张量 N(0,1):\\n{random_normal}\\n\")\n",
    "\n",
    "# 指定范围的随机整数\n",
    "random_int = torch.randint(0, 10, (2, 3))\n",
    "print(f\"随机整数张量 [0,10):\\n{random_int}\\n\")\n",
    "\n",
    "# 类似于另一个张量的随机张量\n",
    "like_tensor = torch.randn_like(matrix.float())\n",
    "print(f\"与matrix相同形状的随机张量:\\n{like_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 数值序列张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 创建数值序列张量：\n",
      "等差数列 arange(0, 10, 2): tensor([0, 2, 4, 6, 8])\n",
      "\n",
      "线性等分 linspace(0, 1, 5): tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "\n",
      "对数等分 logspace(0, 2, 5): tensor([  1.0000,   3.1623,  10.0000,  31.6228, 100.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"📈 创建数值序列张量：\")\n",
    "\n",
    "# 等差数列\n",
    "arange_tensor = torch.arange(0, 10, 2)  # 从0到10，步长为2\n",
    "print(f\"等差数列 arange(0, 10, 2): {arange_tensor}\\n\")\n",
    "\n",
    "# 线性等分\n",
    "linspace_tensor = torch.linspace(0, 1, 5)  # 从0到1，等分成5个点\n",
    "print(f\"线性等分 linspace(0, 1, 5): {linspace_tensor}\\n\")\n",
    "\n",
    "# 对数等分\n",
    "logspace_tensor = torch.logspace(0, 2, 5)  # 10^0 到 10^2，等分成5个点\n",
    "print(f\"对数等分 logspace(0, 2, 5): {logspace_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔍 3. 张量的属性\n",
    "\n",
    "每个张量都有几个重要的属性：\n",
    "\n",
    "- **shape/size**: 张量的形状\n",
    "- **dtype**: 数据类型\n",
    "- **device**: 存储设备(CPU/GPU)\n",
    "- **requires_grad**: 是否需要计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 张量属性详解：\n",
      "tensor([[[ 0.1391, -0.1082, -0.7174,  0.7566,  0.3715],\n",
      "         [-1.0049,  0.0083,  0.3277,  0.2829, -0.8926],\n",
      "         [-0.1626, -0.8062, -0.1168, -1.6124, -0.1541],\n",
      "         [-0.0646, -0.5324,  0.0533, -0.0314, -0.7431]],\n",
      "\n",
      "        [[-1.1582, -0.0249, -0.7584, -0.4157,  0.6389],\n",
      "         [-0.2545, -1.2304, -1.5822,  0.6431,  0.9715],\n",
      "         [-1.3249, -1.0006, -0.4556,  0.4031, -0.7707],\n",
      "         [-1.1757, -0.1555, -0.6527,  0.2520,  0.4590]],\n",
      "\n",
      "        [[ 1.8932,  0.1633, -0.2634, -1.1079,  0.7673],\n",
      "         [-1.1230,  0.3047,  0.3896, -0.2520, -1.0066],\n",
      "         [ 0.2927, -1.1003,  1.9016, -1.5185,  1.6850],\n",
      "         [-1.3713,  0.2893,  0.3939,  0.6529, -0.5519]]], device='cuda:0')\n",
      "📐 形状 (shape): torch.Size([3, 4, 5])\n",
      "📐 大小 (size): torch.Size([3, 4, 5])\n",
      "🔢 维度数 (ndim): 3\n",
      "📊 元素总数 (numel): 60\n",
      "🎭 数据类型 (dtype): torch.float32\n",
      "💻 设备 (device): cuda:0\n",
      "🧮 需要梯度 (requires_grad): False\n",
      "💾 内存布局 (layout): torch.strided\n",
      "📏 步长 (stride): (20, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个示例张量\n",
    "example_tensor = torch.randn(3, 4, 5, dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"🔍 张量属性详解：\")\n",
    "print(example_tensor)\n",
    "\n",
    "print(f\"📐 形状 (shape): {example_tensor.shape}\")\n",
    "print(f\"📐 大小 (size): {example_tensor.size()}\")\n",
    "print(f\"🔢 维度数 (ndim): {example_tensor.ndim}\")\n",
    "print(f\"📊 元素总数 (numel): {example_tensor.numel()}\")\n",
    "print(f\"🎭 数据类型 (dtype): {example_tensor.dtype}\")\n",
    "print(f\"💻 设备 (device): {example_tensor.device}\")\n",
    "print(f\"🧮 需要梯度 (requires_grad): {example_tensor.requires_grad}\")\n",
    "print(f\"💾 内存布局 (layout): {example_tensor.layout}\")\n",
    "print(f\"📏 步长 (stride): {example_tensor.stride()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 数据类型详解\n",
    "\n",
    "PyTorch支持多种数据类型，选择合适的数据类型很重要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 常用数据类型：\n",
      "int32: tensor([1, 2, 3], dtype=torch.int32), dtype: torch.int32\n",
      "long: tensor([1, 2, 3]), dtype: torch.int64\n",
      "float32: tensor([1., 2., 3.]), dtype: torch.float32\n",
      "double: tensor([1., 2., 3.], dtype=torch.float64), dtype: torch.float64\n",
      "bool: tensor([ True, False,  True]), dtype: torch.bool\n",
      "\n",
      "💡 类型转换：\n",
      "int32 → float32: tensor([1., 2., 3.]), dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"🎭 常用数据类型：\")\n",
    "\n",
    "# 整数类型\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(f\"int32: {int_tensor}, dtype: {int_tensor.dtype}\")\n",
    "\n",
    "# 长整数类型 (默认整数类型)\n",
    "long_tensor = torch.tensor([1, 2, 3], dtype=torch.long)\n",
    "print(f\"long: {long_tensor}, dtype: {long_tensor.dtype}\")\n",
    "\n",
    "# 单精度浮点数 (默认浮点类型)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "print(f\"float32: {float_tensor}, dtype: {float_tensor.dtype}\")\n",
    "\n",
    "# 双精度浮点数\n",
    "double_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.double)\n",
    "print(f\"double: {double_tensor}, dtype: {double_tensor.dtype}\")\n",
    "\n",
    "# 布尔类型\n",
    "bool_tensor = torch.tensor([True, False, True], dtype=torch.bool)\n",
    "print(f\"bool: {bool_tensor}, dtype: {bool_tensor.dtype}\")\n",
    "\n",
    "print(\"\\n💡 类型转换：\")\n",
    "# 类型转换\n",
    "converted = int_tensor.float()  # 转换为float32\n",
    "print(f\"int32 → float32: {converted}, dtype: {converted.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 4. 张量的形状操作\n",
    "\n",
    "**形状操作是深度学习中最常用的操作之一，让我们详细学习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 原始张量 x:\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "形状: torch.Size([2, 3, 4])\n",
      "\n",
      "🔄 形状变换操作：\n",
      "reshape(4, 6):\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "形状: torch.Size([4, 6])\n",
      "\n",
      "view(3, 8):\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23]])\n",
      "形状: torch.Size([3, 8])\n",
      "\n",
      "flatten(): tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "形状: torch.Size([24])\n",
      "\n",
      "squeeze前形状: torch.Size([1, 3, 1, 4])\n",
      "squeeze后形状: torch.Size([3, 4])\n",
      "\n",
      "unsqueeze(0)前形状: torch.Size([2, 3, 4])\n",
      "unsqueeze(0)后形状: torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个示例张量\n",
    "x = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"🎯 原始张量 x:\\n{x}\")\n",
    "print(f\"形状: {x.shape}\\n\")\n",
    "\n",
    "print(\"🔄 形状变换操作：\")\n",
    "\n",
    "# 1. reshape: 改变形状但保持元素总数不变\n",
    "reshaped = x.reshape(4, 6)\n",
    "print(f\"reshape(4, 6):\\n{reshaped}\")\n",
    "print(f\"形状: {reshaped.shape}\\n\")\n",
    "\n",
    "# 2. view: 类似reshape但要求内存连续\n",
    "viewed = x.view(3, 8)\n",
    "print(f\"view(3, 8):\\n{viewed}\")\n",
    "print(f\"形状: {viewed.shape}\\n\")\n",
    "\n",
    "# 3. flatten: 展平为1维\n",
    "flattened = x.flatten()\n",
    "print(f\"flatten(): {flattened}\")\n",
    "print(f\"形状: {flattened.shape}\\n\")\n",
    "\n",
    "# 4. squeeze: 移除大小为1的维度\n",
    "squeezed_tensor = torch.randn(1, 3, 1, 4)\n",
    "squeezed = squeezed_tensor.squeeze()\n",
    "print(f\"squeeze前形状: {squeezed_tensor.shape}\")\n",
    "print(f\"squeeze后形状: {squeezed.shape}\\n\")\n",
    "\n",
    "# 5. unsqueeze: 添加大小为1的维度\n",
    "unsqueezed = x.unsqueeze(0)  # 在位置0添加维度\n",
    "print(f\"unsqueeze(0)前形状: {x.shape}\")\n",
    "print(f\"unsqueeze(0)后形状: {unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 维度置换和转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 维度置换操作：\")\n",
    "\n",
    "# 创建一个3维张量 (批次, 高度, 宽度)\n",
    "image_like = torch.randn(2, 28, 28)\n",
    "print(f\"原始形状 (batch, height, width): {image_like.shape}\")\n",
    "\n",
    "# transpose: 交换两个维度\n",
    "transposed = image_like.transpose(1, 2)  # 交换维度1和2\n",
    "print(f\"transpose(1, 2)后形状: {transposed.shape}\")\n",
    "\n",
    "# permute: 重新排列所有维度\n",
    "permuted = image_like.permute(2, 0, 1)  # 重排为 (width, batch, height)\n",
    "print(f\"permute(2, 0, 1)后形状: {permuted.shape}\")\n",
    "\n",
    "# 对于2D矩阵，可以用.T进行转置\n",
    "matrix_2d = torch.randn(3, 4)\n",
    "print(f\"\\n2D矩阵转置：\")\n",
    "print(f\"原始形状: {matrix_2d.shape}\")\n",
    "print(f\"转置后形状: {matrix_2d.T.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧮 5. 张量的数学运算\n",
    "\n",
    "PyTorch提供了丰富的数学运算，支持元素级运算和矩阵运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧮 基本数学运算：\")\n",
    "\n",
    "# 创建两个示例张量\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(f\"张量 a:\\n{a}\")\n",
    "print(f\"张量 b:\\n{b}\\n\")\n",
    "\n",
    "# 元素级运算\n",
    "print(\"🔢 元素级运算：\")\n",
    "print(f\"加法 a + b:\\n{a + b}\\n\")\n",
    "print(f\"减法 a - b:\\n{a - b}\\n\")\n",
    "print(f\"乘法 a * b (元素级):\\n{a * b}\\n\")\n",
    "print(f\"除法 a / b:\\n{a / b}\\n\")\n",
    "print(f\"幂运算 a ** 2:\\n{a ** 2}\\n\")\n",
    "\n",
    "# 矩阵运算\n",
    "print(\"📊 矩阵运算：\")\n",
    "print(f\"矩阵乘法 a @ b:\\n{a @ b}\\n\")\n",
    "print(f\"矩阵乘法 torch.mm(a, b):\\n{torch.mm(a, b)}\\n\")\n",
    "\n",
    "# 广播机制\n",
    "print(\"📡 广播机制：\")\n",
    "scalar = 10\n",
    "print(f\"张量与标量运算 a + {scalar}:\\n{a + scalar}\\n\")\n",
    "\n",
    "vector = torch.tensor([1, 2])\n",
    "print(f\"张量与向量运算 a + {vector}:\\n{a + vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 聚合运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 聚合运算：\")\n",
    "\n",
    "# 创建一个示例张量\n",
    "data = torch.randn(3, 4)\n",
    "print(f\"示例数据:\\n{data}\\n\")\n",
    "\n",
    "# 统计运算\n",
    "print(f\"求和 sum(): {data.sum()}\")\n",
    "print(f\"平均值 mean(): {data.mean():.4f}\")\n",
    "print(f\"最大值 max(): {data.max()}\")\n",
    "print(f\"最小值 min(): {data.min()}\")\n",
    "print(f\"标准差 std(): {data.std():.4f}\")\n",
    "print(f\"方差 var(): {data.var():.4f}\\n\")\n",
    "\n",
    "# 按维度聚合\n",
    "print(\"按维度聚合：\")\n",
    "print(f\"按行求和 sum(dim=1): {data.sum(dim=1)}\")\n",
    "print(f\"按列求和 sum(dim=0): {data.sum(dim=0)}\")\n",
    "print(f\"按行求平均 mean(dim=1): {data.mean(dim=1)}\")\n",
    "\n",
    "# 保持维度\n",
    "print(f\"\\n保持维度 sum(dim=1, keepdim=True):\\n{data.sum(dim=1, keepdim=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 数学函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔬 数学函数：\")\n",
    "\n",
    "# 创建测试数据\n",
    "x = torch.linspace(-2, 2, 5)\n",
    "print(f\"输入 x: {x}\\n\")\n",
    "\n",
    "# 激活函数\n",
    "print(\"🧠 激活函数：\")\n",
    "print(f\"ReLU: {torch.relu(x)}\")\n",
    "print(f\"Sigmoid: {torch.sigmoid(x)}\")\n",
    "print(f\"Tanh: {torch.tanh(x)}\\n\")\n",
    "\n",
    "# 三角函数\n",
    "print(\"📐 三角函数：\")\n",
    "print(f\"Sin: {torch.sin(x)}\")\n",
    "print(f\"Cos: {torch.cos(x)}\\n\")\n",
    "\n",
    "# 指数和对数\n",
    "positive_x = torch.abs(x) + 0.1  # 确保为正数\n",
    "print(\"📈 指数和对数：\")\n",
    "print(f\"指数 exp: {torch.exp(x)}\")\n",
    "print(f\"对数 log: {torch.log(positive_x)}\")\n",
    "print(f\"开方 sqrt: {torch.sqrt(positive_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 6. 索引和切片\n",
    "\n",
    "张量的索引和切片操作类似于NumPy，但有一些PyTorch特有的功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 索引和切片操作：\")\n",
    "\n",
    "# 创建一个3维张量\n",
    "tensor_3d = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"原始3D张量:\\n{tensor_3d}\\n\")\n",
    "\n",
    "print(\"🔍 基本索引：\")\n",
    "print(f\"第0个元素 [0]: \\n{tensor_3d[0]}\\n\")\n",
    "print(f\"第0行第1列 [0, 1]: {tensor_3d[0, 1]}\\n\")\n",
    "print(f\"特定元素 [0, 1, 2]: {tensor_3d[0, 1, 2]}\\n\")\n",
    "\n",
    "print(\"✂️ 切片操作：\")\n",
    "print(f\"前两行 [:2]: \\n{tensor_3d[:2]}\\n\")\n",
    "print(f\"所有行的第1列 [:, 1]: \\n{tensor_3d[:, 1]}\\n\")\n",
    "print(f\"最后两列 [:, :, -2:]: \\n{tensor_3d[:, :, -2:]}\\n\")\n",
    "\n",
    "print(\"🎲 高级索引：\")\n",
    "# 布尔索引\n",
    "mask = tensor_3d > 10\n",
    "masked_values = tensor_3d[mask]\n",
    "print(f\"大于10的元素: {masked_values}\\n\")\n",
    "\n",
    "# 花式索引\n",
    "indices = torch.tensor([0, 1, 0])\n",
    "selected = tensor_3d[indices]\n",
    "print(f\"选择特定索引 [0, 1, 0]: \\n{selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔗 7. 张量的拼接和分割\n",
    "\n",
    "在深度学习中，经常需要组合或分割张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔗 张量拼接和分割：\")\n",
    "\n",
    "# 创建两个张量\n",
    "t1 = torch.randn(2, 3)\n",
    "t2 = torch.randn(2, 3)\n",
    "print(f\"张量1:\\n{t1}\")\n",
    "print(f\"张量2:\\n{t2}\\n\")\n",
    "\n",
    "print(\"🔗 拼接操作：\")\n",
    "# 按行拼接 (dim=0)\n",
    "cat_dim0 = torch.cat([t1, t2], dim=0)\n",
    "print(f\"按行拼接 cat(dim=0):\\n{cat_dim0}\\n\")\n",
    "\n",
    "# 按列拼接 (dim=1)\n",
    "cat_dim1 = torch.cat([t1, t2], dim=1)\n",
    "print(f\"按列拼接 cat(dim=1):\\n{cat_dim1}\\n\")\n",
    "\n",
    "# 堆叠 - 创建新维度\n",
    "stacked = torch.stack([t1, t2], dim=0)\n",
    "print(f\"堆叠 stack(dim=0) 形状: {stacked.shape}\")\n",
    "print(f\"堆叠结果:\\n{stacked}\\n\")\n",
    "\n",
    "print(\"✂️ 分割操作：\")\n",
    "# 等分分割\n",
    "chunks = torch.chunk(cat_dim0, 2, dim=0)\n",
    "print(f\"等分为2块: {len(chunks)}个张量\")\n",
    "print(f\"第一块:\\n{chunks[0]}\\n\")\n",
    "\n",
    "# 按指定大小分割\n",
    "splits = torch.split(cat_dim1, 3, dim=1)\n",
    "print(f\"按大小3分割: {len(splits)}个张量\")\n",
    "print(f\"第一块形状: {splits[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 8. 张量与NumPy的互转\n",
    "\n",
    "PyTorch张量可以与NumPy数组无缝转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 PyTorch与NumPy互转：\")\n",
    "\n",
    "# PyTorch → NumPy\n",
    "pytorch_tensor = torch.randn(2, 3)\n",
    "numpy_array = pytorch_tensor.numpy()  # 注意：共享内存！\n",
    "\n",
    "print(f\"PyTorch张量:\\n{pytorch_tensor}\")\n",
    "print(f\"转换为NumPy:\\n{numpy_array}\")\n",
    "print(f\"NumPy类型: {type(numpy_array)}\\n\")\n",
    "\n",
    "# NumPy → PyTorch\n",
    "numpy_data = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "torch_from_numpy = torch.from_numpy(numpy_data)  # 共享内存\n",
    "torch_tensor = torch.tensor(numpy_data)  # 复制数据\n",
    "\n",
    "print(f\"NumPy数组:\\n{numpy_data}\")\n",
    "print(f\"from_numpy():\\n{torch_from_numpy}\")\n",
    "print(f\"tensor():\\n{torch_tensor}\\n\")\n",
    "\n",
    "# ⚠️ 注意：内存共享的影响\n",
    "print(\"⚠️ 内存共享示例：\")\n",
    "pytorch_tensor[0, 0] = 999\n",
    "print(f\"修改PyTorch张量后的NumPy数组:\\n{numpy_array}\")\n",
    "print(\"👆 注意：NumPy数组也被修改了！(共享内存)\")\n",
    "\n",
    "# 如果张量在GPU上，需要先移到CPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = torch.randn(2, 2).cuda()\n",
    "    cpu_numpy = gpu_tensor.cpu().numpy()\n",
    "    print(f\"\\nGPU张量转NumPy: {cpu_numpy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💻 9. 设备操作 (CPU/GPU)\n",
    "\n",
    "PyTorch可以在不同设备上运行，主要是CPU和GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💻 设备操作：\")\n",
    "\n",
    "# 检查CUDA可用性\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA设备数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"当前CUDA设备: {torch.cuda.current_device()}\")\n",
    "    print(f\"设备名称: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "print(f\"\\n使用设备: {device}\\n\")\n",
    "\n",
    "# 创建张量到指定设备\n",
    "cpu_tensor = torch.randn(2, 3)  # 默认在CPU\n",
    "print(f\"CPU张量设备: {cpu_tensor.device}\")\n",
    "\n",
    "# 移动到GPU (如果可用)\n",
    "gpu_tensor = cpu_tensor.to(device)\n",
    "print(f\"GPU张量设备: {gpu_tensor.device}\")\n",
    "\n",
    "# 直接在GPU创建张量\n",
    "if torch.cuda.is_available():\n",
    "    direct_gpu = torch.randn(2, 3, device='cuda')\n",
    "    print(f\"直接GPU张量设备: {direct_gpu.device}\")\n",
    "\n",
    "# 设备间运算规则\n",
    "print(\"\\n⚠️ 重要：只有相同设备的张量才能运算！\")\n",
    "try:\n",
    "    # 这会出错（如果有GPU的话）\n",
    "    if torch.cuda.is_available():\n",
    "        result = cpu_tensor + direct_gpu  # 不同设备张量相加\n",
    "except RuntimeError as e:\n",
    "    print(f\"错误: {e}\")\n",
    "\n",
    "# 正确做法：确保张量在同一设备\n",
    "result = cpu_tensor.to(device) + gpu_tensor\n",
    "print(f\"\\n正确运算结果设备: {result.device}\")\n",
    "\n",
    "# 内存使用情况 (GPU)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU内存使用:\")\n",
    "    print(f\"已分配: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "    print(f\"已缓存: {torch.cuda.memory_reserved() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 10. 实战练习：图像数据处理\n",
    "\n",
    "让我们用张量操作来处理一个实际的图像数据示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🖼️ 实战：图像数据处理\")\n",
    "\n",
    "# 模拟一批RGB图像数据\n",
    "# 形状: (批次大小, 通道数, 高度, 宽度)\n",
    "batch_size, channels, height, width = 4, 3, 32, 32\n",
    "images = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "print(f\"📊 图像批次信息:\")\n",
    "print(f\"形状: {images.shape}\")\n",
    "print(f\"数据类型: {images.dtype}\")\n",
    "print(f\"数值范围: [{images.min():.3f}, {images.max():.3f}]\\n\")\n",
    "\n",
    "# 1. 数据标准化 (常见预处理)\n",
    "normalized = (images - images.mean()) / images.std()\n",
    "print(f\"📈 标准化后:\")\n",
    "print(f\"均值: {normalized.mean():.6f}\")\n",
    "print(f\"标准差: {normalized.std():.6f}\\n\")\n",
    "\n",
    "# 2. 通道重排序 (BGR → RGB)\n",
    "# 假设原始是BGR，转换为RGB\n",
    "rgb_images = images[:, [2, 1, 0], :, :]  # 交换通道顺序\n",
    "print(f\"🎨 通道重排序后形状: {rgb_images.shape}\\n\")\n",
    "\n",
    "# 3. 图像缩放 (双线性插值)\n",
    "import torch.nn.functional as F\n",
    "resized = F.interpolate(images, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "print(f\"🔍 缩放后形状: {resized.shape}\\n\")\n",
    "\n",
    "# 4. 数据增强：随机翻转\n",
    "def random_flip(images, p=0.5):\n",
    "    \"\"\"随机水平翻转图像\"\"\"\n",
    "    mask = torch.rand(images.size(0)) < p\n",
    "    flipped = images.clone()\n",
    "    flipped[mask] = torch.flip(flipped[mask], dims=[3])  # 沿宽度维度翻转\n",
    "    return flipped\n",
    "\n",
    "augmented = random_flip(images)\n",
    "print(f\"🎲 数据增强后形状: {augmented.shape}\\n\")\n",
    "\n",
    "# 5. 批次统计\n",
    "print(f\"📊 批次统计:\")\n",
    "print(f\"每个图像的均值: {images.mean(dim=[1,2,3])}\")\n",
    "print(f\"每个通道的均值: {images.mean(dim=[0,2,3])}\")\n",
    "print(f\"空间维度的均值: {images.mean(dim=[2,3]).shape}\")\n",
    "\n",
    "# 6. 展平用于全连接层\n",
    "flattened_for_fc = images.view(batch_size, -1)\n",
    "print(f\"\\n🔗 展平用于全连接层: {flattened_for_fc.shape}\")\n",
    "print(f\"每个样本特征数: {flattened_for_fc.size(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 总结与关键要点\n",
    "\n",
    "### 🎯 核心概念\n",
    "1. **张量是PyTorch的基础数据结构**，类似于NumPy数组但支持GPU计算\n",
    "2. **形状操作是关键技能**：reshape, view, transpose, permute等\n",
    "3. **设备管理很重要**：确保张量在正确的设备上进行运算\n",
    "4. **广播机制**让不同形状的张量可以进行运算\n",
    "\n",
    "### 💡 实用技巧\n",
    "- 使用 `print_tensor_info()` 函数检查张量属性\n",
    "- 合理选择数据类型以节省内存\n",
    "- 注意内存共享：`numpy()` vs `tensor()`\n",
    "- GPU张量需要移到CPU才能转换为NumPy\n",
    "\n",
    "### 🚀 下一步学习\n",
    "- 自动求导机制 (Autograd)\n",
    "- 神经网络构建 (nn.Module)\n",
    "- 损失函数和优化器\n",
    "- 数据加载和预处理\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 练习任务\n",
    "\n",
    "完成以下练习来巩固张量操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏆 练习任务：\")\n",
    "\n",
    "# 练习1: 创建一个3x4的随机矩阵，然后:\n",
    "# - 计算每行的和\n",
    "# - 找出最大值及其位置\n",
    "# - 转置矩阵\n",
    "print(\"📝 练习1: 矩阵操作\")\n",
    "matrix = torch.randn(3, 4)\n",
    "print(f\"原矩阵:\\n{matrix}\")\n",
    "\n",
    "# 你的代码在这里...\n",
    "row_sums = matrix.sum(dim=1)\n",
    "max_val, max_idx = matrix.max(), matrix.argmax()\n",
    "transposed = matrix.T\n",
    "\n",
    "print(f\"每行和: {row_sums}\")\n",
    "print(f\"最大值: {max_val}, 位置: {max_idx}\")\n",
    "print(f\"转置后形状: {transposed.shape}\\n\")\n",
    "\n",
    "# 练习2: 创建两个2x3矩阵，进行元素级乘法和矩阵乘法\n",
    "print(\"📝 练习2: 矩阵运算\")\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(3, 2)  # 注意形状以便矩阵乘法\n",
    "\n",
    "# 你的代码在这里...\n",
    "print(f\"A形状: {A.shape}, B形状: {B.shape}\")\n",
    "print(f\"矩阵乘法 A@B 形状: {(A @ B).shape}\")\n",
    "\n",
    "print(\"\\n🎉 恭喜完成张量基础学习！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
